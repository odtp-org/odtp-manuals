{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ODTP Open Digital Twin Project","text":"<p>ODTP is a framework to combine independently developed tools into Digital Twins. </p>"},{"location":"#getting-started-with-odtp","title":"Getting started with ODTP","text":"<p>Install ODTP ODTP Tutorials</p>"},{"location":"#concepts","title":"Concepts","text":"<p>Read about ODTP:</p> <ul> <li>Orchestrator describes the app ODTP and its architecture</li> <li>Components teaches you how to select and build components</li> <li>Workflows describes how to combine components into workflows</li> <li>Zoo describes a registry for components</li> </ul> <pre><code>graph RL;\n    subgraph ODTP\n        direction TB\n        ODTPWorkflows[ODTP Workflows] -.- combine -.-&gt; ODTPComponents[ODTP Components]\n        ODTPOrchestrator[ODTP Orchestrator] -.- executes -.-&gt; ODTPWorkflows[ODTP Workflows]\n        ODTPZoo[ODTP Zoo] -.- registers -.-&gt; ODTPComponents[ODTP Components]\n        subgraph ODTPComponents[ODTP Components]\n            A1[Tool] -- transform --&gt; A2[Component]\n        end\n        subgraph ODTPWorkflows[ODTP Workflows]\n            B1[Components] -- combine --&gt; B2[Workflows]\n        end\n        subgraph ODTPOrchestrator[ODTP Orchestrator]\n            C1[Digital Twins &gt; Executions of Workflows]\n            C2[(S3:Snapshots of Data)]\n            C3[(Mongo DB: Operational Metadata)]\n            C1 --&gt; C2\n            C1 --&gt; C3\n        end\n        subgraph ODTPZoo[ODTP Zoo]\n            D1[Components] -- register --&gt; D2[Index]\n        end\n    end\nstyle ODTPComponents fill:white\nstyle ODTPWorkflows fill:white\nstyle ODTPOrchestrator fill:white\nstyle ODTPZoo fill:white</code></pre>"},{"location":"components/","title":"ODTP Components","text":""},{"location":"components/#what-is-a-component-in-odtp","title":"What is a Component in ODTP?","text":"<p>Components are the way tools are made available in ODTP: </p> <ul> <li>the tool can be any github repo, that performs a function that is necessary for a Digital Twin, such as loading, processing or visualizing data, see Component types</li> <li>the Component is a second github repo, that checks out a version of the tool and is enables the ODTP orchestrator to run the tool in a Docker container within ODTP, so that the execution of the tool is monitored by ODTP and the outputs are captured and can be used by other ODTP components, so that pipelines can be established. </li> </ul> <pre><code>graph LR;\n    Tool --&gt;|transform| ODTPComponent\n    subgraph ODTPComponent[Component]\n    Component[ODTP Client]\n    Commit[checkout tool version]\n    Metadata[odtp.yml]\n    Docker[Dockerfile]\n    end</code></pre>"},{"location":"components/#component-requirements","title":"Component Requirements","text":"<p>An ODTP Component needs the following parts:</p> <ul> <li>A Dockerfile: that builds the Component as Docker image, so that it can be run by the ODTP Orchestrator</li> <li>An App script: that will be started in the Docker container. It runs the tool and communicates its outputs, results and logs to the ODTP orchestrator</li> <li>Checkout tool version: the tool will be checked out in <code>app.sh</code> from a git repo on a specified version.</li> <li>An ODTP client library is installed in the Dockerfile, so that the App script can use predefined functions to communicate with the ODTP orchestrator</li> </ul> <pre><code>graph LR;\n    ODTPComponent --&gt;|run| DockerContainer\n    subgraph ODTPComponent[Component]\n    Client[Dockerfile: mount ODTP Client]\n    App[App: call ODTP Client]\n    Docker[Dockerfile: Start App]\n    end</code></pre>"},{"location":"components/#component-as-a-blackbox","title":"Component as a Blackbox","text":"<p>You can think of the Component as a blackbox that takes inputs files and/or parameters to perfom a task. Usually this leads to some files as a result (Ephemeral component), or to a visualization (Interactive component), see Component types</p> <p>Develop a Component</p>"},{"location":"components/develop/","title":"How to develop a component","text":"<p>Here you find instructions on how to turn an existing tool into an ODTP component:</p> <p>Note</p> <p>Not every component needs to come from an external tool: you can also develop a component that is unrelated to a tool</p> <pre><code>graph LR;\n    Tool -.-&gt;|transform| ODTPComponent\n    Template --&gt;|use| ODTPComponent\n    subgraph ODTPComponent[Component]\n    Component[ODTP Client]\n    Commit[checkout tool version]\n    Metadata[odtp.yml]\n    Docker[Dockerfile]\n    end</code></pre> <ul> <li>How to develop a component<ul> <li>Step 1: Use the odtp-component-template to create a github repo for the component</li> <li>Step 2: Adapt the Dockerfile and Installations</li> <li>Step 3: Adapt the app/app.sh</li> <li>Step 4: Provide Metadata for the Component</li> <li>Step 5: Test the component<ul> <li>Testing the component as a docker container</li> <li>Testing the component as part of odtp</li> </ul> </li> <li>Step 6: Version your Component</li> <li>Step 7: Publish your tool in the ODTP Zoo.</li> </ul> </li> </ul>"},{"location":"components/develop/#step-1-use-the-odtp-component-template-to-create-a-github-repo-for-the-component","title":"Step 1: Use the <code>odtp-component-template</code> to create a github repo for the component","text":"<p>Start with the ODTP component template:</p> <ul> <li>go to <code>odtp-component-template</code>\u00a0\u29c9</li> <li>Click on \"Use this template\": \"Open a new repository\"</li> <li>Give the component a name similar to \"odtp-your-tool-name\"</li> </ul> <p>Note</p> <p>This repository makes use of submodules. Therefore, when cloning it you need to include them.</p> <pre><code>git clone --recurse-submodules https://github.com/your-organization/odtp-your-tool-name\n</code></pre> <p>See README\u00a0\u29c9</p> <p>The resulting repo has the following structure:</p> <pre><code>\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 README.template.md\n\u251c\u2500\u2500 app\n\u2502  \u251c\u2500\u2500 app.sh\n\u2502  \u2514\u2500\u2500 config_templates\n\u2502       \u2514\u2500\u2500 template.yml\n\u251c\u2500\u2500 odtp-component-client\n\u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 logger.py\n\u2502   \u251c\u2500\u2500 odtp-app.sh\n\u2502   \u251c\u2500\u2500 parameters.py\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u251c\u2500\u2500 s3uploader.py\n\u2502   \u2514\u2500\u2500 startup.sh\n\u251c\u2500\u2500 odtp.yml\n\u251c\u2500\u2500 .env.dist\n\u2514\u2500\u2500 requirements.txt\n</code></pre> <p>Files and Folders that need to get modified:</p> <ul> <li><code>app</code>: contains <code>app.sh</code> and additional configuration for it: this script will checkout and run your tool. It will get started by the <code>startup.sh</code> that is part of the <code>odtp-component-client</code> and serves as an entrypoint for the Dockerfile.</li> <li><code>Dockerfile</code>: check whether you need additional installations for your tool to run and add this to the Dockerfile: you may also use <code>requirements.txt</code> in case your tool is in python</li> <li><code>odtp.yml</code>: this will contain the metadata for your component</li> <li><code>.env.dist</code>: includes the environment variables that your component needs and that are also specified in <code>odtp.yml</code></li> <li><code>README.md</code>: there is a template <code>README.template.md</code> that you can started on making a <code>README.md</code> that describes your component.</li> </ul> <p>Mounted as git submodule (should not be modify):</p> <ul> <li><code>odtp-component-client</code>: client for the odtp orchestrator: <code>startup.sh</code> serves as an entrypoint in Docker</li> </ul> <p>All changes will be further described in the steps below. So no need to do them now already.</p>"},{"location":"components/develop/#step-2-adapt-the-dockerfile-and-installations","title":"Step 2: Adapt the Dockerfile and Installations","text":"<p>In this step you will adapt the build instructions for the Docker Image:</p> <ul> <li>If your tool runs on python: adapt the <code>requirements.txt</code> file and add the libraries that you need.</li> <li>Adapt the Dockerfile and install the needed libraries that your tool needs to run:</li> </ul> <pre><code>FROM ubuntu:22.04\n\nRUN apt update\nRUN apt install python3.10 python3-pip -y\n\n##################################################\n# Ubuntu setup\n##################################################\n\nRUN  apt-get update \\\n  &amp;&amp; apt-get install -y wget \\\n  &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN apt-get update &amp;&amp; apt-get -y upgrade \\\n  &amp;&amp; apt-get install -y --no-install-recommends \\\n    unzip \\\n    nano \\\n    git \\\n    g++ \\\n    gcc \\\n    htop \\\n    zip \\\n    ca-certificates \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n##################################################\n# ODTP setup\n##################################################\n\nCOPY odtp-component-client/requirements.txt /tmp/odtp.requirements.txt\nRUN pip install -r /tmp/odtp.requirements.txt\n\n\n#######################################################################\n# PLEASE INSTALL HERE ALL SYSTEM DEPENDENCIES RELATED TO YOUR TOOL\n#######################################################################\n\n# Installing dependencies from the app\nCOPY requirements.txt /tmp/requirements.txt\nRUN pip install -r /tmp/requirements.txt\n</code></pre> <p>Don't touch the second part of the Dockerfile, from this line onwards:</p> <pre><code>######################################################################\n# ODTP COMPONENT CONFIGURATION.\n# DO NOT TOUCH UNLESS YOU KNOW WHAT YOU ARE DOING.\n######################################################################\n</code></pre>"},{"location":"components/develop/#step-3-adapt-the-appappsh","title":"Step 3: Adapt the app/app.sh","text":"<p>Change the section below so that a version of your tool is checked out:</p> <p>Clone the repository of your tool and checkout to one specific commit. We recommend to specify the commit with a tag for a semantic version.</p> <pre><code>#########################################################\n# 1. GITHUB CLONING OF REPO\n# Clone the repository of your tool and checkout to one specific commit.\n#########################################################\n\n# git clone https://github.com/odtp-org/tool-example.git /odtp/odtp-workdir/tool-example\n# cd /odtp/odtp-workdir/tool-example\n# git checkout xxxxxxxxxxxx\n</code></pre> <p>(Optional) If your app uses a config file (i.e. <code>config.yml</code> or <code>config.json</code>), you need to provide a templace including placeholders for the variables you would like to expose. Placeholders can be defined by using double curly braces wrapping the name of the variable, such as <code>{{VARIABLE}}</code>. Then you can run <code>python3 /odtp/odtp-component-client/parameters.py PATH_TO_TEMPLATE PATH_TO_OUTPUT_CONFIG_FILE</code> and every placeholder will be replaced by the value in the environment variable.</p> <pre><code>#########################################################\n# 2. CONFIG FILE CONFIGURATION\n# Read placeholders and create config file from Environment\n#########################################################\n\n# python3 /odtp/odtp-component-client/parameters.py /odtp/odtp-app/config_templates/template.yml /odtp/odtp-workdir/config.yml\n</code></pre> <p>Copy (<code>cp -r</code>) or create symbolic links (<code>ln -s</code>) to locate the input files in <code>/odpt/odtp-input/</code> in the folder.</p> <pre><code>#########################################################\n# 3. INPUT FOLDER MANAGEMENT\n#########################################################\n\n# ln -s /odtp/odtp-input/... /odtp/odtp-workdir/...\n</code></pre> <p>Run the tool. You can access to the parameters as environment variables (i.e. <code>$PARAMETER_A</code>)</p> <pre><code>#########################################################\n# 4. TOOL EXECUTION\n# While the output is managed by ODTP and placed in /odtp/odtp-output/\n#########################################################\n\n# COMMAND $PARAMETER_A #PARAMETER_B /odtp/odtp-input/data\n</code></pre> <p>Manage the output exporting. At the end of the component execution all generated output should be located in <code>/odtp/odtp-output</code>. Copy all output files into this folder.</p> <pre><code>#########################################################\n# 5. OUTPUT FOLDER MANAGEMENT\n# The selected output files generated should be placed in the output folder\n#########################################################\n\n# cp -r /odtp/odtp-workdir/output/* /odtp/odtp-output\n</code></pre>"},{"location":"components/develop/#step-4-provide-metadata-for-the-component","title":"Step 4: Provide Metadata for the Component","text":"<p>ODTP requires a set of metadata to work that it is defined in a file called <code>odtp.yml</code> that should be in the root of the repository. As the file is parsed when a Component Version is added to ODTP. Therefore it is important to  conform to the schema when filling in the <code>odtp.yml</code> file, see Specifcation on <code>odtp.yml</code></p>"},{"location":"components/develop/#step-5-test-the-component","title":"Step 5: Test the component","text":"<p>There are 3 main ways in which you can test a component and the different odtp features.</p> <ol> <li>Testing it as a docker container</li> <li>Testing it as a single component using <code>odtp</code></li> <li>Testing it in a <code>odtp</code> digital twin execution</li> </ol> <p>When developing we recommend to start by testing the component via docker and then follow with the others.</p>"},{"location":"components/develop/#testing-the-component-as-a-docker-container","title":"Testing the component as a docker container","text":"<p>The user will need to manually create the input/output folders and build the docker image.</p> <p>Prepare the following folder structure:</p> <pre><code>- testing-folder\n    - data-input\n    - data-output\n</code></pre> <p>Place all required input files in <code>testing-folder/data-input</code>.</p> <p>In case you have parameters specified in the <code>odtp.yaml</code> file:</p> <ul> <li><code>cp .env.dist .env</code></li> <li>Create your <code>.env</code> file with the following parameters. If you don't have parameters you can omit this.</li> </ul> <pre><code># ODTP COMPONENT VARIABLES\nPARAMETER-A=.....\nPARAMETER-B=.....\n</code></pre> <p>Build the dockerfile.</p> <pre><code>docker build -t odtp-component .\n</code></pre> <p>Run the following command.</p> <pre><code>docker run -it --rm \\\n-v {PATH_TO_YOUR_INPUT_VOLUME}:/odtp/odtp-input \\\n-v {PATH_TO_YOUR_OUTPUT_VOLUME}:/odtp/odtp-output \\\n--env-file .env \\\nodtp-component\n</code></pre> <p>This command will run the component. If you want debug some errors and execute the docker in an interactive manner, you can use the flag <code>--entrypoint bash</code> when running docker.</p> <p>Also if your tool is interactive such as an Streamlit\u00a0\u29c9 app, don't forget to map the ports by using <code>-p XXXX:XXXX</code>.</p>"},{"location":"components/develop/#testing-the-component-as-part-of-odtp","title":"Testing the component as part of odtp","text":"<p>For this you need to have odtp installed: see Install ODTP Then run the component as described in How to tun a single component</p>"},{"location":"components/develop/#step-6-version-your-component","title":"Step 6: Version your Component","text":"<p>ODTP relies on tagged versions of Component. In the ODTP Orchestrator you need a version tag for the Component to register it. Use Semantic Versioning\u00a0\u29c9 for your Component. For instance: <code>v0.1.0</code> or <code>v0.2.0-alpha</code>.</p>"},{"location":"components/develop/#step-7-publish-your-tool-in-the-odtp-zoo","title":"Step 7: Publish your tool in the ODTP Zoo.","text":"<p>Once your component has been tested you can publish it in the ODTP Zoo. See Add component to the ODTP-org zoo</p>"},{"location":"components/find/","title":"Find Components","text":"<p>In the future validated and available components are supposed to be listed by the ODTP zoo. But not all Components are listed there yet.</p> <p>Components can also be found at the ODTP organization\u00a0\u29c9. The repo name usually starts with <code>odtp-</code> such as the <code>odtp-component-example</code>\u00a0\u29c9.</p>"},{"location":"components/find/#odtp-component-example","title":"ODTP Component example","text":"<pre><code>graph LR;\n    Tool --&gt;|transform| ODTPComponent\n    subgraph ODTPComponent[Component]\n    Commit[checkout tool version]\n    end</code></pre> <p>In order to give and example we have provided an example component:</p> <ul> <li>ODTP Component Example\u00a0\u29c9</li> </ul> <p>This component wraps the following tool: </p> <ul> <li>Tool Example\u00a0\u29c9</li> </ul> <p>This component is of type Dataloader.</p>"},{"location":"components/find/#use-cases","title":"Use Cases","text":"<p>Find more example in the Use Cases.</p>"},{"location":"components/odtp-yml/","title":"odtp.yml Format","text":"<p>Note</p> <p>it is very important to fill the odtp.yml file correctly as in the version v0.5.0 this file has become of operational importance: * it is is used to prefill parameters and ports * it imports metadata on the Component Version into ODTP</p> <p>Below you find an description of how to fill in the <code>odtp.yml</code> file:</p> odtp.yml<pre><code># Schema version for tracking updates to the schema format\nschema-version: \"v0.5.0\"\n\n# Component Information\ncomponent-name: Component Name\ncomponent-authors:\n  - name: Author One\n    orcid: \"https://orcid.org/0000-0001-2345-6789\"\n  - name: Author Two\n    orcid: \"https://orcid.org/0000-0002-3456-7890\"\ncomponent-version: \"1.0.0\"\ncomponent-repository:\n  url: \"https://github.com/organization/component-repo\"\n  doi: \"https://doi.org/10.1234/component.doi\"\ncomponent-license: Component License\ncomponent-type: ephemeral or interactive\ncomponent-description: Description of the component's function\ncomponent-docker-image: \"dockeruser/dockerimage:label\"\ntags:\n  - tag1\n  - tag2\n\n# Tool Information\ntools:\n  - tool-name: Tool Name\n    tool-authors:\n      - name: Tool Author\n        orcid: \"https://orcid.org/0000-0001-1234-5678\"\n    tool-version: Tool Version\n    tool-repository:\n      url: \"https://github.com/organization/tool-repo\"\n      doi: \"https://doi.org/10.1234/tool.doi\"\n    tool-license: Tool License\n\n# Secrets (ENV variables)\nsecrets:\n  - name: API_KEY\n    description: API key for authentication\n    type: str\n\n# Build Arguments (if any)\nbuild-args:\n  - name: MATLAB_LICENSE\n    description: License key for Matlab\n    secret: true # Mark as secret if sensitive\n\n# Exposed Ports\nports:\n  - name: PORT_A\n    description: Main server port\n    port-value: 8080\n  - name: PORT_B\n    description: Auxiliary service port\n    port-value: 9090\n\n# Parameters for the Component\nparameters:\n  - name: PARAMETER_A\n    default-value: 10\n    datatype: int\n    description: Max retries allowed\n    parameter-bounds:\n      - 0 # Minimum value\n      - 100 # Maximum value\n    options: null\n    allow-custom-value: false\n\n  - name: PARAMETER_B\n    default-value: OptionA\n    datatype: str\n    description: Select a mode\n    options:\n      - OptionA\n      - OptionB\n      - OptionC # Limited choices for str type\n    allow-custom-value: false\n\n# Data Inputs\ndata-inputs:\n  - name: INPUT_A\n    type: .txt\n    path: /path/to/input/SIMPLE_INPUT.txt\n    description: Single static input file\n    naming-convention: \"SIMPLE_INPUT.txt\"\n\n  - name: INPUT_B\n    type: TYPE_B\n    path: /path/to/input/folder_A\n    description: Folder containing dynamically named input files\n    naming-convention: \"data_{PARAMETER_A}_{PARAMETER_B}_v{number}.ext\"\n    dynamic-naming-based-on:\n      - PARAMETER_A\n      - PARAMETER_B\n    sequence:\n      start: 1\n      increment: 1\n\n  - name: INPUT_C\n    type: TYPE_C\n    path: /path/to/input/folder_B\n    description: Folder with structured input files\n    folder-structure:\n      required-files:\n        - file-pattern: \"summary_{PARAMETER_C}_{date}.txt\"\n        - file-pattern: \"log_{PARAMETER_C}_{number}.json\"\n      naming-convention: \"parameter_and_numeric_based\"\n      dynamic-naming-based-on:\n        - PARAMETER_C\n      date-format: \"YYYYMMDD\"\n      sequence:\n        start: 1\n        increment: 1\n\n# Data Outputs\ndata-outputs:\n  - name: OUTPUT_A\n    type: .txt\n    path: /path/to/output/SIMPLE_OUTPUT.txt\n    description: Static output file\n    naming-convention: \"SIMPLE_OUTPUT.txt\"\n\n  - name: OUTPUT_B\n    type: TYPE_B\n    path: /path/to/output/folder_A\n    description: Folder for dynamic output files\n    naming-convention: \"prefix_{PARAMETER_A}_{PARAMETER_B}_v{number}.ext\"\n    dynamic-naming-based-on:\n      - PARAMETER_A\n      - PARAMETER_B\n    sequence:\n      start: 1\n      increment: 1\n\n  - name: OUTPUT_C\n    type: TYPE_C\n    path: /path/to/output/folder_B\n    description: Folder for structured output files\n    folder-structure:\n      required-files:\n        - file-pattern: \"output_summary_{PARAMETER_C}_{date}.txt\"\n        - file-pattern: \"log_{PARAMETER_C}_{number}.json\"\n      naming-convention: \"parameter_and_numeric_based\"\n      dynamic-naming-based-on:\n        - PARAMETER_C\n      date-format: \"YYYYMMDD\"\n      sequence:\n        start: 1\n        increment: 1\n\n# Validation Schemas (Future Development)\nschema-input: PATH_TO_INPUT_SCHEMA\nschema-output: PATH_TO_OUTPUT_SCHEMA\n\n# Device Requirements\ndevices:\n  - type: gpu\n    required: true\n</code></pre>"},{"location":"components/run/","title":"How to run a single component?","text":"<p>This section assumes that you have installed ODTP. If this is not the case:</p> <p>Install ODTP</p>"},{"location":"components/run/#how-to-run-a-component-with-the-odtp-orchestrator","title":"How to run a component with the ODTP Orchestrator","text":"<p>Components can only be run on the Command Line Interface (CLI), not from the Graphical User Interface(GUI).</p> <p>In this example we are going to run ODTP component example\u00a0\u29c9. First, we will prepare the component which will automatically download the repostory, build the image and prepare all the folders needed for the input / output data. </p> <p>First let's create a project folder called <code>digital_twin_project</code> on your local computer. In this folder is where all the folders will appear. </p> <pre><code>mkdir digital_twin_project\n</code></pre> <p>Then we can prepare the project by running the following. This will download the repo and build the image and prepare the output folder structure</p> <pre><code>odtp component prepare \\\n--folder /Users/carlosvivarrios/pro/odtp/digital_twin_project \\\n--image_name image_test \\\n--repository https://github.com/odtp-org/odtp-component-example\n</code></pre> <p>Now we need to run the component: </p> <pre><code>odtp component run \\\n--folder /Users/carlosvivarrios/pro/odtp/digital_twin_project \\\n--image_name image_test \\\n--instance_name instance_test \\\n--repository https://github.com/odtp-org/odtp-component-example \\\n--commit 6471218336ce7de41a5162c9556c0ff68f9ec13c \\\n--parameter_file /Users/carlosvivarrios/pro/odtp/digital_twin_project/.env\n</code></pre> <p>Then we can delete the instance by running. In docker terminology this will remove the container</p> <pre><code>odtp component delete-instance --instance_name instance_test\n</code></pre> <p>And finally if we want to delete the image we can run:</p> <pre><code>odtp component delete-image --image_name image_test \n</code></pre>"},{"location":"components/run/#how-to-use-a-component-in-a-digital-twin","title":"How to use a component in a Digital Twin?","text":"<p>To start using component in Digital Twin's executions, first you need to register the component into odtp, and using it. </p> <p>See the tutorial on how to do this.</p>"},{"location":"components/types/","title":"Types","text":""},{"location":"components/types/#types-of-components","title":"Types of components","text":"<p>ODTP offers the following types of components:</p> <pre><code>mindmap\n  root((Component Types and Examples))\n    Component Type: Ephemeral\n      Dataloader\n      Analytical\n      Data Exporter\n    Component Type: Interactive\n      Visualization\n      Dashboard\n    Component Type: API\n      Service that is difficult to set up and is therefore shared</code></pre>"},{"location":"components/types/#ephemeral-components","title":"Ephemeral components","text":"<pre><code>flowchart LR\n    DockerBuild[Build Docker Image] --&gt; DockerRun[Run Docker Container]\n    DockerRun[Run Docker Container] --&gt; Stop[Stop]</code></pre> <p>Ephemeral components are docker container that run once and exit after they have run the tool that they wrap. The docker container is discarded after use.</p> <p>Examples for Ephemeral components are:</p> <ul> <li>Dataloader: loading data from various sources</li> <li>Data Analysis / data sciene: analyzing or transforming data for statistic or predictive analysis</li> <li>Data Exporter: export to external data storage</li> </ul>"},{"location":"components/types/#interactive-components","title":"Interactive components","text":"<pre><code>flowchart LR\n    DockerBuild[Build Docker Image] --&gt; DockerRun[Run Docker Container]</code></pre> <p>Interactive components are designed to interact with the user. Their docker container will be kept running until the user stops it. They usually have inputs (file or database) and they usually provide a webservice that is run on a port, that will be exposed, so that the user can change it.</p> <p>Examples for Interactive components are:</p> <ul> <li>Dashboards</li> <li>Visualizations</li> </ul>"},{"location":"components/types/#api-components","title":"API components","text":"<p>Under Construction</p> <ul> <li>Api components are currently under construction and not yet available</li> </ul> <pre><code>flowchart LR\n    DockerRun[Run Docker Container]</code></pre> <p>An API component by itself is similar to an interactive component: It mounts a long running service that will only end when termintated by the user.</p> <p>API components differ though in the way they are handled by the ODTP orchestrator: Components are combined into an Workflow and then the workflow can be executed by a Execution. Usually the docker container for a component is build per execution, when the execution is prepared. But with API components they will be build independently of the workflows and executions that they are part of and will provide long lasting services that can still be integrated into workflows and executions with out the need to build the component</p> <p>This kind of component is useful when the component building process (in docker)  takes a large amount of time, or when a long-lasting task can be reused in multiple executions. An example of this is the loading of a machine learning model into memory.</p> <p>The API component receives the parameters as JSON in the request\u2019s payload. This allows a more complex configuration of parameters than in the other two component types. Input data can be provided in the request, or, if the file-size is big, as a item in the S3 storage.</p>"},{"location":"contributing/","title":"Contributing to ODTP","text":"<p>You can contribute to the ODTP Project in the following ways:</p> <ul> <li>Contributing to ODTP<ul> <li>Raise an Issue</li> <li>Make a Pull Requests<ul> <li>Step 1: Discuss your proposal</li> <li>Step 2: Make the Pull Request</li> </ul> </li> <li>Comment on our Documentation</li> </ul> </li> </ul>"},{"location":"contributing/#raise-an-issue","title":"Raise an Issue","text":"<p>Communicate with us via our github repo\u00a0\u29c9.  There you can: </p> <ul> <li>propose enhancements</li> <li>report bugs</li> </ul> <p>We are interested to learn about your use case and how we can improve ODTP to support you.</p>"},{"location":"contributing/#make-a-pull-requests","title":"Make a Pull Requests","text":"<p>If you want propose a improvement of the code base, please do the following:</p>"},{"location":"contributing/#step-1-discuss-your-proposal","title":"Step 1: Discuss your proposal","text":"<p>Raise an issue and describe what you want to work on at the ODTP github repo\u00a0\u29c9 and wait for a response by the ODTP maintainers to get feedback on your proposal. Once an agreement has been reached, move to step 2.</p>"},{"location":"contributing/#step-2-make-the-pull-request","title":"Step 2: Make the Pull Request","text":"<p>Fork the repo ODTP github repo\u00a0\u29c9 and open a Pull Request on the <code>develop</code> branch of the repo. Ask for a review of your Pull Request for getting it merged into the code base.</p>"},{"location":"contributing/#comment-on-our-documentation","title":"Comment on our Documentation","text":"<p>We have an Hypothesis Overlay\u00a0\u29c9 on our documentation.</p> <p>To comment on this documentation please follow the following steps:</p> <ul> <li>follow the steps of the Hyothesis get started guide\u00a0\u29c9</li> <li>Return to the documentation\u00a0\u29c9 and start commenting.</li> </ul>"},{"location":"home/","title":"ODTP Introduction","text":""},{"location":"home/#about-odtp","title":"About ODTP","text":"<p>ODTP is a framework that allows to combine tools that have been developed independently from each other into Digital Twins.</p> <p>OTDP originated in the mobility sector, see Acknowledgement and Funding to enable the composition of Digital Twins. But the framework can be used for any purpose where independent tools need to be combined into workflows.</p>"},{"location":"home/#odtp-as-a-workflow-manager","title":"ODTP as a workflow manager","text":"<p>ODTP's features as workflow manager are the following:</p> <ul> <li>integrate heterogenous tools in the way they have been developed </li> <li>wrap these tools and transform them into Components to make them usable by the ODTP Orchestrator</li> <li>combine Components into Workflows and run them as Executions</li> <li>monitor the executed Workflows and capture outputs, logs and configuration</li> <li>enhance interoperability by using sematic validation of inputs and outputs of Executions</li> <li>group Workflows into Digital Twins and allow to compare results between its executions</li> <li>register Components and Workflows in an ODTP Zoo to make them discoverable and reusable</li> </ul> <p>ODTP has both a Commandline Interface and a Graphical User Interface. It uses S3 to store outputs and Mongodb to capture Components, Workflows and Execution Metadata.</p> <p>Note</p> <p>Not all features of ODTP are yet operational. It is a POC, that is still under development. We encourage you to use it an give us feedback, see Contributing</p> <pre><code>graph RL;\n    subgraph ODTP     \n        direction TB\n        ODTPWorkflows -.- combine -.-&gt; ODTPComponents\n        ODTPOrchestrator -.- executes -.-&gt; ODTPWorkflows\n        ODTPZoo -.- registers -.-&gt; ODTPComponents\n        subgraph ODTPComponents\n            A1[Tool] -- transform --&gt; A2[Component]\n        end\n        subgraph ODTPWorkflows\n            B1[Components] -- combine --&gt; B2[Workflows]\n        end\n        subgraph ODTPOrchestrator\n            C1[Digital Twins &gt; Executions of Workflows]\n            C2[(S3:Snapshots of Data)]\n            C3[(Mongo DB: Operational Metadata)]\n            C1 --&gt; C2\n            C1 --&gt; C3\n        end\n        subgraph ODTPZoo\n            D1[Components] -- register --&gt; D2[Index]\n        end           \n    end\nstyle ODTPComponents fill:white\nstyle ODTPWorkflows fill:white\nstyle ODTPOrchestrator fill:white\nstyle ODTPZoo fill:white </code></pre>"},{"location":"home/#getting-started-with-odtp","title":"Getting started with ODTP","text":"<p>Depending on your interest we recommend the following ways to get started with ODTP:</p> <p>In order to be able to use the framework you need to install it first:</p> <p>Install ODTP</p> <p>If you have have already installed ODTP, we have a tutorial that teaches you  how to use ODTP:</p> <p>Use ODTP: Getting started Tutorial</p>"},{"location":"home/#odtp-concepts","title":"ODTP Concepts","text":"<p>If you want to understand the framework:</p> <ul> <li>Orchestrator describes the app ODTP and its architecture</li> <li>Components teaches you how to select and build components</li> <li>Workflows describes how to combine components into workflows</li> <li>Zoo describes a registry for components and how to add components there</li> </ul>"},{"location":"installation/","title":"Installation Options","text":"<ul> <li>Prerequisites</li> <li>Installation Options</li> <li>Service Dependencies</li> </ul>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>In order to install ODTP you will need to download and install Docker\u00a0\u29c9 in your machine, and git\u00a0\u29c9.</p>"},{"location":"installation/#installation-options","title":"Installation Options","text":"<p>There are two ways of installing ODTP:</p> <ul> <li>Easy installation via Docker Compose</li> <li>Installation for permanent usage on a local server</li> </ul> <p>Note</p> <p>The installation on a local server is more complicated and requires you to also setup the required extra services such as mongodb and S3</p> <p>Easy install with docker compose (Recommended)</p> <p>Install on a local server</p>"},{"location":"installation/#service-dependencies","title":"Service Dependencies","text":"<p>The picture below shows the services that are needed by ODTP:</p> <pre><code>graph TD;\n    subgraph ODTP\n    CLI[CLI]\n    GUI[GUI]\n    end\n    ODTP --&gt;|requires| MongoDB\n    ODTP --&gt;|requires| Minio\n    MongoDBExpress --&gt;|optional| MongoDB\nstyle ODTP fill:white</code></pre>"},{"location":"installation/docker-compose_install/","title":"Install with docker compose","text":"<p>Note</p> <p>For test and development purposes we recommend to use the dockerized ODTP. However this makes use of \"Docker in Docker\" to run the digital executions which can lead to security issues. More information here\u00a0\u29c9.</p> <p>For the setup with docker compose the following is provided at https://github.com/odtp-org/odtp\u00a0\u29c9</p> <ul> <li><code>compose.yml</code> production docker compose setup that is used by default with <code>docker compose</code></li> <li><code>.env.dist</code> explaining the env variables and should be copied to <code>.env</code></li> <li><code>compose.dev.yml</code> development setup of  docker compose: can be used via <code>docker compose -f</code>compose.dev.yml`</li> </ul> <p>The Installation  is done in the following steps:</p> <ul> <li>Prerequisites</li> <li>1. Set up a folder configuration</li> <li>2. Get your your Github Token</li> <li>3. Clone the ODTP Repository</li> <li>4. Setup the environment variables</li> <li>5. Test the docker compose configuration</li> <li>6. Build the docker containers</li> <li>7. Run the docker containers</li> <li>8. ODTP initial configuration</li> <li>9. Ready to use ODTP</li> </ul>"},{"location":"installation/docker-compose_install/#prerequisites","title":"Prerequisites","text":"<p>In order to install ODTP with Docker compose you need Docker compose\u00a0\u29c9 additionally to Docker\u00a0\u29c9 and git\u00a0\u29c9.</p>"},{"location":"installation/docker-compose_install/#1-set-up-a-folder-configuration","title":"1. Set up a folder configuration","text":"<p>Create a folder (we recommend you name it <code>odtp</code>) where ODTP will locate all services, and files needed. Create the following sub-folders: <code>mongodb</code>, <code>minio</code>, and <code>digital-twins</code>. This folders will serve as volumes for the odtp services.</p> <p>The file system structure should look like this:</p> <pre><code>\u2514\u2500\u2500 odtp\n    \u251c\u2500\u2500 digital-twins\n    \u251c\u2500\u2500 minio\n    |   \u2514\u2500\u2500 odtp\n    \u2514\u2500\u2500 mongodb\n</code></pre> <p>Save the name of the path names for later use during the installation:</p> <ul> <li>absolute path to the <code>digital-twins</code> folder as <code>[ODTP_PATH]</code></li> <li>absolute path to the <code>minio</code> folder as <code>[MINIO_PATH]</code></li> <li>absolute path to the <code>mongodb</code> folder as <code>[MONGODB_PATH]</code></li> </ul> <p>Note</p> <p>Windows users need to use Linux style syntax such as: <code>ODTP_PATH=/c/odtp/digitaltwins</code></p>"},{"location":"installation/docker-compose_install/#2-get-your-your-github-token","title":"2. Get your your Github Token","text":"<p>To complete the configuration of the <code>.env</code> we will need your Github Token:</p> <p>Go to the Github Token page\u00a0\u29c9 and generate a new classic token with full access rights. Choose an appropriate expiration data to work with the token.</p> <p>Save the name of the <code>[GITHUB_TOKEN]</code> for later use during the installation.</p>"},{"location":"installation/docker-compose_install/#3-clone-the-odtp-repository","title":"3. Clone the ODTP Repository","text":"<p>Pull the ODTP\u00a0\u29c9 repository. We recommend to do it in the same folder you created before (e.g.. <code>odtp</code>)</p> <pre><code>git clone https://github.com/odtp-org/odtp.git\n</code></pre> <p>Afterwards your folder structure will look like this:</p> <pre><code>\u2514\u2500\u2500 odtp\n    \u251c\u2500\u2500 digital-twins\n    \u251c\u2500\u2500 minio\n    |   \u2514\u2500\u2500 odtp\n    \u2514\u2500\u2500 mongodb\n    \u2514\u2500\u2500 odtp\n        \u2514\u2500\u2500 compose.yml\n        \u2514\u2500\u2500 .env.dist.compose\n        ...\n</code></pre>"},{"location":"installation/docker-compose_install/#4-setup-the-environment-variables","title":"4. Setup the environment variables","text":"<p>The <code>.env</code> should be completed by adding administrator users, passwords and configuration for the different services:</p> <pre><code>cd odtp\ncp .env.dist .env\n</code></pre> <p>Then fill in your credentials into <code>.env</code> as follows: you can delete parts of the <code>.env</code> file that are only needed for the setup method <code>[VM]</code>. In case you are not setting up the project for development, you can also delete the sections that are only needed for <code>[DEV]</code>.</p> <p>Decide on credentials for the different services</p> <ul> <li>user and password for the Mongodb: <code>[MONGO_DB_USER]</code>, <code>[MONGO_DB_PASSWORD]</code></li> <li>user and password for Mongodb Express: <code>[MONGO_EXPRESS_USER]</code>, <code>[MONGO_EXPRESS_PASSWORD]</code></li> <li>user and password for Minio: <code>[MINIO_ROOT_USER]</code>, <code>[MINIO_ROOT_PASSWORD]</code></li> </ul> <p>You also need a password [ODTP_PASSWORD] for the encrpytion and decryption of secrets</p> <p>For a docker compose production setup, your <code>.env</code> file will look like this:</p> <pre><code># ===========================================================\n# Environment variables for odtp\n# Setup options:\n# - [COMPOSE] `docker compose` (recommended)\n# - [DEV] `docker compose -f compose.dev.yml` (for development)\n# - [VM] server setup with poetry\n# ===========================================================\n\n# ===========================================================\n# Credentials with other services\n# - needed for all setup methods\n# ===========================================================\n\n# Credentials github\n# your github token\nGITHUB_TOKEN=[GITHUB_TOKEN]\n\n# ===========================================================\n# Credentials for odtp: to decrypt secret files\n# ===========================================================\n\nODTP_PASSWORD=[ODTP_PASSWORD]\n\n# ===========================================================\n# Credentials that you can choose on setup\n# - needed for all setup methods\n# ===========================================================\n\n# Credentials S3\nODTP_ACCESS_KEY=[MINIO_ROOT_USER]\nODTP_SECRET_KEY=[MINIO_ROOT_PASSWORD]\n\n# mongodb user and password\nMONGO_DB_USER=[MONGO_DB_USER]\nMONGO_DB_PASSWORD=[MONGO_DB_PASSWORD]\n\n# mongoexpress user and password\nMONGO_EXPRESS_USER=[MONGO_EXPRESS_USER]\nMONGO_EXPRESS_PASSWORD=[MONGO_EXPRESS_PASSWORD]\n\n# ===========================================================\n# Database names\n# - needed for all setup methods\n# ===========================================================\n\n# odtp db instance in the mongo db: \"odtp\"\nODTP_MONGO_DB=odtp\n\n# s3 bucket name: \"odtp\"\nODTP_BUCKET_NAME=odtp\n\n# ===========================================================\n# ODTP Path is where the dashboard will store users,\n# digital twins and executions\n# - needed for all setup methods\n# ===========================================================\n\n# path where your executions run and the digital twins are stored\nODTP_PATH=[ODTP_PATH]\n\n# ===========================================================\n# Volumes to persist database content\n# these must match path on your local computer\n# - only needed for setup methods [COMPOSE], [DEV]\n# ===========================================================\n\n# path where s3 data is stored\nMINIO_PATH=[MINIO_PATH]\n\n# path where mongodb content is stored\nMONGODB_PATH=[MONGODB_PATH]\n\n# ===========================================================\n# Operational settings: change only if needed\n# ===========================================================\n\n# Dashboard parameters\n# you can chose a different port to serve the dashboard in case port\n# 8003 is not available as port on your computer\nODTP_DASHBOARD_PORT=8003\n# this setting should only be True during development but False in\n# production\nODTP_DASHBOARD_RELOAD=False\n\n# Log level\n# Log level for the dashboard\nODTP_LOG_LEVEL=ERROR\n# log level for the component runs\nRUN_LOG_LEVEL=INFO\n\n# Set to False if your docker installation does not allow the flag --gpus all\n# Set to True in case you want to use GPUs\nALLOW_DOCKER_GPUS=False\n</code></pre> <p>Note</p> <p>The variables <code>APP_PATH</code> and <code>PIP_INSTALL_ARGS=\"--editable\"</code> are only needed for the development setup of the project with <code>docker compose -f compose.dev.yml</code>. In that case <code>APP_PATH</code> should be the path to the project directory on your local computer. For a production setup this section of the <code>.env</code> file can be deleted.</p>"},{"location":"installation/docker-compose_install/#5-test-the-docker-compose-configuration","title":"5. Test the docker compose configuration","text":"<p>Test your configuration:</p> <pre><code>docker compose config\n</code></pre> <p>This will print out a generated <code>docker-compose.yml</code> file as it will be used for the <code>docker compose up</code>.</p>"},{"location":"installation/docker-compose_install/#6-build-the-docker-containers","title":"6. Build the docker containers","text":"<p>Run the docker compose build command in the <code>odtp</code> directory where the <code>compose.yml</code> file resides: The build might take up to 30 minutes. As long as it is progressing from step to step it should be fine.</p> <pre><code>docker compose build --no-cache\n</code></pre>"},{"location":"installation/docker-compose_install/#7-run-the-docker-containers","title":"7. Run the docker containers","text":"<p>Once the containers have been build, you can run them. You need to repeat this step each time you start your computer. In case you just want to update the environment variables in the <code>.env</code> file, you can use the flag <code>--force-recreate</code>. The <code>-d</code> flag means that the containers are going to be run in detached mode. If you want to see the logs on the terminal, omit this flag</p> <pre><code>docker compose up --force-recreate -d\n</code></pre>"},{"location":"installation/docker-compose_install/#8-odtp-initial-configuration","title":"8. ODTP initial configuration","text":"<p>Enter the Docker container <code>odtp-odtp-1</code> and execute: <code>odtp setup initiate</code>. This will finish the configuration of the database and s3 instance:</p> <pre><code>docker exec -it odtp-odtp-1 odtp setup initiate\n</code></pre>"},{"location":"installation/docker-compose_install/#9-ready-to-use-odtp","title":"9. Ready to use ODTP","text":"<p>Now you are ready use <code>ODTP</code> directly via the CLI or via the GUI:</p> <p>Start with the Commandline Interface of ODTP:</p> <pre><code>docker exec -it odtp-odtp-1 sh\n</code></pre> <p>Start using the ODTP Dashboard:</p> <pre><code>docker exec -it odtp-odtp-1 odtp dashboard\n</code></pre> <p>ODTP Tutorials</p>"},{"location":"installation/local_server_install/","title":"Install on a local server","text":"<p>Note</p> <p>For deploying digital twins we recommend to use ODTP as a local installation as described here. In a production environment running digital twins, a local installation on the operating system is preferable. Note that this requires additional third party services to be setup along side ODTP</p> <ul> <li>Prerequisites<ul> <li>Docker</li> <li>Git</li> <li>Mongodb</li> <li>Minio</li> <li>Github token</li> <li>Python 3.11</li> <li>Poetry</li> <li>Docker network</li> </ul> </li> <li>1. Install ODTP</li> <li>2. Set the environment variables:</li> <li>3. Install ODTP and dependencies locally</li> <li>4 Configure MongoDB and S3 bucket</li> <li>5. How to test that everything works?</li> <li>Services and Ports</li> </ul>"},{"location":"installation/local_server_install/#prerequisites","title":"Prerequisites","text":"<p>In order to install ODTP you need</p>"},{"location":"installation/local_server_install/#docker","title":"Docker","text":"<ul> <li>Docker\u00a0\u29c9</li> </ul>"},{"location":"installation/local_server_install/#git","title":"Git","text":"<ul> <li>git\u00a0\u29c9</li> </ul> <p>On a local server, ODTP needs additionally the following services to be setup:</p>"},{"location":"installation/local_server_install/#mongodb","title":"Mongodb","text":"<ul> <li>Mongodb\u00a0\u29c9</li> </ul> <p>Form this installation you need the following variables:</p> <ul> <li><code>[MONGO_DB_USER]</code> and <code>[MONGO_DB_PASSWORD]</code> and <code>[MONGO_SERVER_URL]</code> from the Mongodb Installation</li> </ul> <p>Optional: for the Mongodb it is advisable to install also a graphical user interface for the mongodb. Options are:</p> <ul> <li>https://www.mongodb.com/products/tools/compass</li> <li>https://github.com/mongo-express/mongo-express</li> </ul>"},{"location":"installation/local_server_install/#minio","title":"Minio","text":"<ul> <li>Minio\u00a0\u29c9</li> </ul> <p>From these installations you need the following variables:</p> <ul> <li><code>[MINIO_ROOT_USER]</code> and <code>[MINIO_ROOT_PASSWORD]</code> and <code>[MINIO_URL]</code> from the Minio Installation</li> </ul>"},{"location":"installation/local_server_install/#github-token","title":"Github token","text":"<p>You need a github token:</p> <p>Go to the Github Token page\u00a0\u29c9 and generate a new classic token with full access rights. Choose an appropriate expiration data to work with the token.</p> <ul> <li>Save the name of the <code>[GITHUB_TOKEN]</code> for later use during the installation. You also need your <code>[GITHUB_USER]</code></li> </ul>"},{"location":"installation/local_server_install/#python-311","title":"Python 3.11","text":"<p>You also need Python3.11 installed on your Server: https://www.python.org/downloads/. Save the <code>[PYTHON_3_11_PATH]</code> on your local server.</p>"},{"location":"installation/local_server_install/#poetry","title":"Poetry","text":"<p>As  python dependency manager ODTP uses poetry\u00a0\u29c9</p>"},{"location":"installation/local_server_install/#docker-network","title":"Docker network","text":"<p>ODTP needs a Docker network to be setup:</p> <pre><code>docker network create odtp_odtp-network\n</code></pre>"},{"location":"installation/local_server_install/#1-install-odtp","title":"1. Install ODTP","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/odtp-org/odtp.git\ncd odtp\n</code></pre> <p>Install ODTP</p> <pre><code>poetry env use [PYTHON_3_11_PATH]\npoetry shell\npoetry install\n</code></pre> <p>Note</p> <p>On a server with Apple Chip you might need to change the shell using this command: <code>env /usr/bin/arch -x86_64 /bin/bash --login</code> before the installation.</p>"},{"location":"installation/local_server_install/#2-set-the-environment-variables","title":"2. Set the environment variables:","text":"<p>Create a <code>.env</code> file from the template <code>.env.dist.local</code></p> <pre><code>cd odtp\ncp .env.dist .env\n</code></pre> <p>By now you should have:</p> <ul> <li><code>[MINIO_ROOT_USER]</code> and <code>[MINIO_ROOT_PASSWORD]</code> and <code>[MINIO_URL]</code> from the Minio Installation</li> <li><code>[MONGO_DB_USER]</code> and <code>[MONGO_DB_PASSWORD]</code> and <code>[MONGO_SERVER_URL]</code> from the Mongodb Installation</li> <li><code>[GITHUB_TOKEN]</code> and <code>[GITHUB_USER]</code> from your github</li> </ul> <p>Additionally you need:</p> <ul> <li>Minio Bucket name: <code>[MINIO_BUCKET_NAME]</code>, recommended <code>odtp</code></li> <li>Minio Bucket name: <code>[MONGO_DB_NAME]</code>, recommended <code>odtp</code></li> <li>Working directory: <code>[ODTP_PATH]</code>: working directory for user of ODTP, where projects and data files can be stored</li> </ul> <p>You also need a password [ODTP_PASSWORD] for the encrpytion and decryption of secrets</p> <p>Delete the parts in the <code>.env</code> file that are not needed for the setup method [VM].</p> <p>For a server setup, your <code>.env</code> file will look like this:</p> <pre><code># ===========================================================\n# Environment variables for odtp\n# Setup options:\n# - [COMPOSE] `docker compose` (recommended)\n# - [DEV] `docker compose -f compose.dev.yml` (for development)\n# - [VM] server setup with poetry\n# ===========================================================\n\n# ===========================================================\n# Credentials with other services\n# - needed for all setup methods\n# ===========================================================\n\n# Credentials github\n# your github token\nGITHUB_TOKEN=[GITHUB_TOKEN]\n\n# ===========================================================\n# Credentials for odtp: to decrypt secret files\n# ===========================================================\n\nODTP_PASSWORD=[ODTP_PASSWORD]\n\n# ===========================================================\n# Credentials that you can choose on setup\n# - needed for all setup methods\n# ===========================================================\n\n# Credentials S3\nODTP_ACCESS_KEY=[MINIO_ROOT_USER]\nODTP_SECRET_KEY=[MINIO_ROOT_PASSWORD]\n\n# mongodb user and password\nMONGO_DB_USER=[MONGO_DB_USER]\nMONGO_DB_PASSWORD=[MONGO_DB_PASSWORD]\n\n# ===========================================================\n# Operational settings: change only if needed\n# - needed for all setup methods\n# ===========================================================\n\n# odtp db instance in the mongo db: \"odtp\"\nODTP_MONGO_DB=odtp\n\n# s3 bucket name: \"odtp\"\nODTP_BUCKET_NAME=odtp\n\n# ===========================================================\n# ODTP Path is where the dashboard will store users,\n# digital twins and executions\n# - needed for all setup methods\n# ===========================================================\n\n# path where your executions run and the digital twins are stored\nODTP_PATH=[ODTP_PATH]\n\n# ===========================================================\n# Development settings: only needed for development\n# setup with compose.dev.yml:\n# `docker compose -f compose.dev.yml`\n# - only needed for setup method [DEV]\n# ===========================================================\n\n# Dashboard parameters\n# you can chose a different port to serve the dashboard in case port\n# 8003 is not available as port on your computer\nODTP_DASHBOARD_PORT=8003\n# this setting should only be True during development but False in\n# production\nODTP_DASHBOARD_RELOAD=False\n\n# Log level\n# Log level for the dashboard\nODTP_LOG_LEVEL=ERROR\n# log level for the component runs\nRUN_LOG_LEVEL=INFO\n\n# Set to False if your docker installation does not allow the flag --gpus all\n# Set to True in case you want to use GPUs\nALLOW_DOCKER_GPUS=False\n\n# ===========================================================\n# Service URLS\n# - only needed for setup method [VM]\n# ===========================================================\n\n# mongo url: example \"mongodb://localhost:27017/\"\nODTP_MONGO_SERVER=[MONGO_SERVER_URL]\n\n# S3 server url: example: \"https://s3.epfl.ch\"\nODTP_S3_SERVER=[MINIO_URL]\n</code></pre> <p>ODTP will use the .env file to access the services and github. Please make sure that you have entered all information correctly.</p> <p>Note</p> <p>Under OSX, you may be asked to accept the type change through renaming of the .env file. If you don't accept, it will be named .env but will still be of type .env.dist meaning the installation will fail. This can be fixed by creating a new file named .env with the same contents.</p> <p>The <code>[ODTP_S3_SERVER]</code> requires the http://</p>"},{"location":"installation/local_server_install/#3-install-odtp-and-dependencies-locally","title":"3. Install ODTP and dependencies locally","text":"<p>We provide installation of ODTP via poetry:</p> <ol> <li>(Required for OSX) Run <code>poetry env use 3.11</code>.</li> <li>Run <code>poetry install</code></li> <li>Run <code>poetry shell</code></li> <li>Run <code>odtp --help</code></li> </ol> <p>This should print out the help for <code>odtp --help</code></p> <p>Note</p> <ul> <li>For OSX, the environment needs to be set to 3.11 because there is no wheel for duckdb on arm64 (Apple Silicon) for python 3.12 and higher (last checked April 1, 2024). Poetry selects python 3.12 because there is no dependency issues but missing wheels are not accounted for.</li> </ul>"},{"location":"installation/local_server_install/#4-configure-mongodb-and-s3-bucket","title":"4 Configure MongoDB and S3 bucket","text":"<p>After deploying the different services you need to run the following command in order to finish the configuration of the collections required.</p> <pre><code>odtp setup initiate\n</code></pre> <p>This command will create the collections needed in mongoDB and S3 automatically.</p>"},{"location":"installation/local_server_install/#5-how-to-test-that-everything-works","title":"5. How to test that everything works?","text":"<p>In order to do some test we can create first an user:</p> <pre><code>odtp new user-entry \\\n--name Pedro \\\n--email vote@for.pedro \\\n--github pedro\n</code></pre> <pre><code>user ID: 65c3648260106cc50f650bc1\n</code></pre> <p>Now that everything has been set up, you are ready to work. Head over to the tutorials.</p> <p>ODTP Tutorials</p>"},{"location":"installation/local_server_install/#services-and-ports","title":"Services and Ports","text":"<p>Below, you can see an overview of the dependencies of services required to run ODTP. Ports are defined by default. Please adjust them according to your installation.</p> <pre><code>graph TD;\n    subgraph ODTP\n    CLI[CLI]\n    GUI[GUI in port 8000]\n    end\n    ODTP --&gt;|requires| MongoDBInstance\n    ODTP --&gt;|requires| MinioInstance\n    subgraph MongoDBInstance[MongoDB Instance]\n    MongoDB[API in port 27017]\n    end\n    subgraph MongoDBExpress[MongoDB Express]\n    MDBEGUI[GUI in port 8081]\n    end\n    subgraph MinioInstance[Minio Instance]\n    MinioAPI[API in port 9000]\n    MinioGUI[GUI in port 9001]\n    end\n    MongoDBExpress --&gt;|dashboard for| MongoDBInstance</code></pre>"},{"location":"legal/acknowledgement/","title":"Acknowledgements and Funding","text":"<p>This work is part of the broader project Open Digital Twin Platform of the Swiss Mobility System (ODTP-SMS) funded by Swissuniversities CHORD grant Track B - Establish Projects. </p> <p>ODTP-SMS project is a joint endeavour by  - the Center for Sustainable Future Mobility - CSFM (ETH Z\u00fcrich) - the Swiss Data Science Center - SDSC (EPFL and ETH Z\u00fcrich). </p> <p>The Swiss Data Science Center (SDSC) develops domain-agnostic standards and containerized components to manage digital twins. This includes the creation of the Core Platform (both back-end and front-end), Service Component Integration Templates, Component Ontology, and the Component Zoo template. </p> <p>The Center for Sustainable Future Mobility (CSFM) develops mobility services and utilizes the components produced by SDSC to deploy a mobility digital twin platform. CSFM focuses on integrating mobility services and collecting available components in the mobility zoo, thereby applying the digital twin concept in the realm of mobility.</p>"},{"location":"legal/copyright/","title":"Copyright","text":"<p>Copyright \u00a9 2023-2024 Swiss Data Science Center (SDSC), www.datascience.ch. All rights reserved. The SDSC is jointly established and legally represented by the \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL) and the Eidgen\u00f6ssische Technische Hochschule Z\u00fcrich (ETH Z\u00fcrich). This copyright encompasses all materials, software, documentation, and other content created and developed by the SDSC.</p>"},{"location":"legal/copyright/#intellectual-property-ip-rights","title":"Intellectual Property (IP) Rights","text":"<p>The Open Digital Twin Platform (ODTP) is the result of a collaborative effort between ETH Zurich (ETHZ) and the \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL). Both institutions hold equal intellectual property rights for the ODTP project, reflecting the equitable and shared contributions of EPFL and ETH Z\u00fcrich in the development and advancement of this initiative.  </p>"},{"location":"legal/disclaimer/","title":"Ethical Use and Legal Compliance Disclaimer","text":"<p>Please note that this software should not be used to harm any individual or entity. Users and developers must adhere to ethical guidelines and use the software responsibly and legally. This disclaimer serves to remind all parties involved in the use or development of this software to engage in practices that are ethical, lawful, and in accordance with the intended purpose of the software.</p>"},{"location":"legal/licensing/","title":"Licensing","text":"<p>The core component of the ODTP software is distributed as open-source under the AGPLv3 license. This ensures that all modifications and derivatives remain open source, fostering a collaborative and shared development environment. Detailed terms of the AGPLv3 license can be found in the LICENSE file within this distribution package.</p>"},{"location":"legal/licensing/#distinct-licensing-for-other-components","title":"Distinct Licensing for Other Components","text":"<p>Service Component Integration Templates are licensed under the BSD-3 license, allowing for broad compatibility and standardization. Ontology: The foundational ODTP ontology is licensed under the Creative Commons Attribution-ShareAlike (CC BY-SA), promoting open use and ensuring that any derivatives also remain open. Component Zoo Template: The template for the Component Zoo operates under the BSD-3 license, emphasizing broad compatibility and open development. It's important to note that individual components within the Zoo retain their original licenses.</p>"},{"location":"legal/licensing/#alternative-commercial-licensing","title":"Alternative Commercial Licensing","text":"<p>Alternative commercial licensing options for the core platform and other components are available and can be negotiated through the EPFL Technology Transfer Office\u00a0\u29c9 or ETH Z\u00fcrich Technology Transfer Office\u00a0\u29c9.</p>"},{"location":"orchestrator/","title":"ODTP Orchestrator Overview","text":"<p>The Orchestrator is the Application behind ODTP: it can be installed and run.</p> <pre><code>graph TD;\n    ODTPOrchestrator --&gt; |defines and runs| Executions\n    ODTPOrchestrator --&gt; |registers| ODTPComponents\n    ODTPOrchestrator --&gt; |registers| Workflows\n    ODTPOrchestrator --&gt; |provides service to| Users\n    ODTPOrchestrator --&gt; |registers| DigitalTwins\n    Users --&gt; |own| DigitalTwins\n    DigitalTwins --&gt; |are collections of| Executions\n    Executions --&gt; |correspond to| Workflows\n    Executions --&gt; |are configured pipelines of| ODTPComponents\n    Workflows --&gt; |are unconfigured pipelines of| ODTPComponents\n    subgraph ODTPOrchestrator\n    Minio[S3: store snapshots between component runs]\n    Mongodb[Mongodb: store all Metadata]\n    Docker[Docker: run executions]\n    end</code></pre> <p>The ODTP orchestrator registers ODTP Components allows to combine them into executable workflows and to run these executions as docker containers.</p> <p>This section explains that concepts behind ODTP and its technical implementation.</p>"},{"location":"orchestrator/#concepts","title":"Concepts","text":"<ul> <li>Concept</li> <li>Architecture</li> <li>Schema</li> <li>Roadmap</li> </ul>"},{"location":"orchestrator/#use-the-orchestrator","title":"Use the orchestrator","text":"<p>In order to use the ODTP orchestrator you need first to install it.</p> <p>Install ODTP</p> <p>In case you have it already installed: the tutorials will guide you on how to use it:</p> <p>Tutorials: get started with ODTP</p>"},{"location":"orchestrator/architecture/","title":"ODTP Architecture","text":"<pre><code>graph TD;\n    subgraph ODTP\n    GUI[Graphical User Interface] --&gt; ODTPCore\n    CLI[Commandline Interface] --&gt; ODTPCore\n    ODTPCore[Core: register components, run executions, register Digital Twins for users]\n    end\n    ODTP --&gt;|store operational data| MongoDBInstance\n    ODTP --&gt;|store data outputs of executions| MinioInstance\n    ODTP --&gt;|engine| Docker\n    subgraph MongoDBInstance[MongoDB Instance]\n    MongoDB[Operational Data]\n    end\n    subgraph MinioInstance[Minio S3]\n    MinioS3[Capture Snapshots of Outputs]\n    end\n    subgraph Docker[Docker as engine]\n    DockerInstance[run components via Docker]\n    end</code></pre> <p>The architecture of the odtp include different core-modules dealing with specific task. Between parenthesis you can find the technologies that are being considered for this modules.</p> <ul> <li>GUI / Dashboard Nicegui\u00a0\u29c9</li> <li>CLI Python: Typer\u00a0\u29c9</li> <li>Snapshots/Data transferring Minio S3\u00a0\u29c9</li> <li>Operational Data Mongodb\u00a0\u29c9, see Schema</li> </ul> <p>See Roadmap for planned enhancements: Roadmap</p>"},{"location":"orchestrator/concept/","title":"Concept","text":""},{"location":"orchestrator/concept/#combining-tools-into-workflows","title":"Combining tools into workflows","text":"<p>The concept of ODTP starts with tools that have been deveoped independently: they may be in different programming languages and have not be designed to run in combination. ODTP offers a solutions to combine these tools in pipeline without permanently linking them or integrating them into a monolytic application.</p> <p><pre><code>---\ntitle: Independet Tools can be combined into pipelines by turning them into Components\n---\ngraph LR;\n    ComponentA[Component A: Example Dataloader] --&gt;|run| ComponentB[Component B: Example ML Model]\n    ComponentB[Component B: Example ML Model] --&gt;|run| ComponentC[Component C: Example Dashboard]</code></pre> <pre><code>graph TD;\n    ToolA[Tool A: Example Dataloader]\n    ToolB[Tool B: Example ML Model]\n    ToolC[Tool C: Example Dashboard]</code></pre></p> <p>ODTP does this by turning the tools into components that are compatible and can be run by the ODTP Orchestrator. Each tool runs as an independent service (as Docker Container). Data between Components can be transferred via the help of Data Snapshots. Input and Outputs are semantically described and can be validated using Shacl. Not all these features have been implemented yet, see architecture and roadmap for the current status.</p>"},{"location":"orchestrator/concept/#classes-in-the-odtp","title":"Classes in the ODTP:","text":"<ul> <li>DigitalTwins: One or more Executions are managed as a Digital Twin. The Executions of a Digital Twin can be rerun of similar or different workflows, that are grouped by a common goal or Use Case. Digital Twins are owned by Users.</li> <li>Users: are the Users of ODTP. They define and then own Executions and Digital Twins. Components are not owned by Users but shared by all Users and just added once with updates of their Versions</li> <li>Components: Components are the building blocks: they come in Versions that correspond to the versions of the tool that they wrap or versions of the Component code. Read about ODTP Components</li> <li>Workflows: Components can be combined into Workflows that are acyclic graphs (currently graphs have to be linear). Read about ODTP Workflows</li> <li>Executions: These workflows are run as Executions. In an Executions each Component becomes a Step.</li> </ul> <pre><code>---\ntitle: User defined Executions of a Digital Twin that share a common result\n---\nflowchart LR\n  subgraph ODTP\n    subgraph User\n        direction TB\n        subgraph DigitalTwin\n            direction LR\n            subgraph ExecutionA\n                direction LR\n                Step1A --&gt; Step2A\n                Step2A --&gt; Step3A\n                subgraph Step1A[Step1A = Component1]\n                end\n                subgraph Step2A[Step2A = Component2]\n                end\n                subgraph Step3A[Step3A = Component3]\n                end\n            end\n            subgraph ExecutionB\n                direction LR\n                Step1B --&gt; Step2B\n                Step2B --&gt; Step3B\n                subgraph Step1B[Step1B = Component1]\n                end\n                subgraph Step2B[Step2B = Component2]\n                end\n                subgraph Step3B[Step3B = Component3]\n                end\n            end\n            ExecutionA --&gt; Result\n            ExecutionB --&gt; Result\n            Result --&gt; Visualize\n            subgraph Result\n            end\n            subgraph Visualize\n            end\n        end\n    end\n    subgraph MongoDB\n    end\n    subgraph S3\n    end\n  end\nDigitalTwin -- Operational data of all steps --&gt; MongoDB\nDigitalTwin -- Transfers of Data between steps--&gt; S3\nstyle ODTP fill:white</code></pre>"},{"location":"orchestrator/concept/#tools-can-be-turned-into-odtpcomponents","title":"Tools can be turned into ODTPComponents","text":"<p>The tools can be of the following kinds:</p> <ul> <li>Data loading</li> <li>Data preparation</li> <li>Data analysis</li> <li>Data visualization</li> </ul> <p>See component Types.</p>"},{"location":"orchestrator/concept/#odtp-executes-workflows-that-combine-components","title":"ODTP executes Workflows that combine Components","text":"<p>The tools that have been transformed into components can then be used in Workflows: ODTP aims to support workflows that are acyclic graphs, but so far only linear pipelines are supported.</p> <pre><code>graph RL;\n    subgraph ODTP\n        direction LR\n        ODTPComponentA --&gt; ODTPComponentB\n        ODTPComponentB --&gt; ODTPComponentC\n        subgraph ODTPComponentA[ComponentA]\n            AAdapter[ODTPAdapter]\n            ATool[ToolA]\n        end\n        subgraph ODTPComponentB[ComponentB]\n            BAdapter[ODTPAdapter]\n            BTool[ToolB]\n        end\n        subgraph ODTPComponentC[ComponentC]\n            CAdapter[ODTPAdapter]\n            CTool[ToolB]\n        end\n    end</code></pre>"},{"location":"orchestrator/roadmap/","title":"Roadmap","text":"<p>ODTP has been setup as a POC. With the experience of ODTP our learning have been captured in the Open Digital Twin Component Standard ODTWS\u00a0\u29c9.</p> <p>The roadmap below explains possible extensions of ODTP that bring it closer to an implementation of ODTWS.</p> <ul> <li>User Authentication and Authorization</li> <li>Semantic Output Validation</li> <li>Shared Results within a Digital Twin</li> <li>Checker for compatibility of licenses</li> <li>Support of complex components</li> <li>Zoo integration</li> <li>Export Workflows as compose files</li> <li>Performance and efficiency improvements</li> <li>Scheduler for workflow executions</li> <li>Compatibility with the Swiss Data Custodian</li> <li>Support Directed Acyclic Graphs</li> </ul>"},{"location":"orchestrator/roadmap/#user-authentication-and-authorization","title":"User Authentication and Authorization","text":"<p>Even though ODTP has user owned parts such as Digital Twins and Executions, Authentication hasn't been added yet. We plan to implement it using Keycloak and the OpenID Connect protocol.</p>"},{"location":"orchestrator/roadmap/#semantic-output-validation","title":"Semantic Output Validation","text":"<p>ODTP plans to provide the sematic input and output validation for the data that are exchanged between components and stored in the S3. That way it can be made sure in an automated way that components are compatible with each other.</p> <ul> <li>automatically extract semantic information about a directory (recursively analyze the files in a folder, and extract some metadata about the columns/properties of each file) and write it to a RDF graph.</li> <li>This \u201cinstance data\u201d (metadata about the input dataset) will be validated against the \u201cSchema data\u201d provided by an Component.</li> <li>The validation be implemented  using SHACL\u00a0\u29c9. The ODTP Orchestrator will run the SHACL validation engine on this combination to generate a report which provides information about the conformance of the dataset with the schema.</li> <li>the Orchestrators will warn users of non-conforming component connections. it may deny the execution of a workflow that is not conforming. The mode for warning will be called \u201cLazy execution\u201d, where a workflow may run until an error occurs. The mode for strict conformance will be called \u201cSafe execution\u201d, where a workflow is not run if non-conforming.</li> </ul>"},{"location":"orchestrator/roadmap/#shared-results-within-a-digital-twin","title":"Shared Results within a Digital Twin","text":"<p>Some step outputs can be collected and arranged as results. These special outputs will be considered in later stages for multi-execution analysis or data visualizations. The implementation of Results will be changed so that they span multiple executions of a Digital Twin. That way they can be used for multi-execution analysis or data visualizations.</p>"},{"location":"orchestrator/roadmap/#checker-for-compatibility-of-licenses","title":"Checker for compatibility of licenses","text":"<p>Since ODTP components originate from independent tools, it is not clear whether their licenses are compatible with each other. The license checker is an app that checks whether components have licenses that can be combined. It is planned to add a license checker to ODTP.</p>"},{"location":"orchestrator/roadmap/#support-of-complex-components","title":"Support of complex components","text":"<p>API-based persistent components will be added as component types</p>"},{"location":"orchestrator/roadmap/#zoo-integration","title":"Zoo integration","text":"<p>While a Component's metadata is now fetched automatically from <code>odtp.yml</code> in the ODTP Orchestrator, the orchestrator is not yet connected to the ODTP Zoo. Currently the metadata is fetched from the <code>odtp.yml</code> file in the component's github repository.</p>"},{"location":"orchestrator/roadmap/#export-workflows-as-compose-files","title":"Export Workflows as compose files","text":"<p>The goal of ODTP is to prioritize the flexibility and independence of the Digital Twins generated. That's why we are developing the exporting of each digital twin into an <code>odtp-compose.yml</code> file that can be executed directly by Docker. This file will offer the user the possibility to run one specific execution in an isolated and reproducible environment, ensuring consistency across different executions and facilitating collaboration and sharing among users. The <code>odtp-compose.yml</code> will include definitions for all the services involved in the execution, their configuration, inter-service dependencies, and any necessary environment variables or volume mounts.</p> <p>While some use cases already have a docker-compose.yml file, it is not yet automatically generated.</p>"},{"location":"orchestrator/roadmap/#performance-and-efficiency-improvements","title":"Performance and efficiency improvements","text":"<p>I/O operations optimization and enhancement of the Mongodb Schema are possible improvements</p>"},{"location":"orchestrator/roadmap/#scheduler-for-workflow-executions","title":"Scheduler for workflow executions","text":"<p>Another possible enhancement would be to schedule Execution runs.</p>"},{"location":"orchestrator/roadmap/#compatibility-with-the-swiss-data-custodian","title":"Compatibility with the Swiss Data Custodian","text":"<p>An integration of ODTP with the Swiss Data Custodian\u00a0\u29c9 might make sense, so that ODTP can better support use cases with sensitive data.</p>"},{"location":"orchestrator/roadmap/#support-directed-acyclic-graphs","title":"Support Directed Acyclic Graphs","text":"<p>ODTP should be able to support not only linear workflows but also workflows that resemble directed acyclic graphs.</p>"},{"location":"orchestrator/schema/","title":"MongoDB Schema","text":"<p>The operational metadata of ODTP is stored in a MongoDB. In this section the schema of the MongoDB is explained by differentiating between the following parts:</p> <ul> <li>building material in ODTP: Components and Component Versions</li> <li>registration of Executions</li> <li>running of Executions</li> </ul>"},{"location":"orchestrator/schema/#er-diagram-for-building-material","title":"ER Diagram for building material:","text":"<p>The first ER diagram describes all mongodb collections that relate to building executions, these can also be found in the ODTP Zoo and are shared building blocks that are not owned by a user:</p> <ul> <li>A Component corresponds to a tool and there is usually a github repository for each component</li> <li>A ComponentVersion is a tagged version of the Component in its github repository</li> <li>A Workflow has multiple Component Versions in a field <code>versions</code>: the sequence matters. Currrently only linear workflows are possible with some tweaks, but supporting acyclic graphs is on the roadmap</li> </ul> <pre><code>---\ntitle: Mongo DB Schema for Building Material\n---\nerDiagram\n    Workflow ||..|{ ComponentVersion : has\n    Components ||..|{ ComponentVersion : has</code></pre> <p>Components have several Versions. Component Versions can be used to build Workflow, that can then be executed as Executions.</p>"},{"location":"orchestrator/schema/#er-diagram-for-the-registration-of-executions","title":"ER Diagram for the registration of executions:","text":"<p>The second ER diagram describes all mongodb collections that relate to the registration of an execution before it is run:</p> <ul> <li>A User can have zero or more DigitalTwins</li> <li>A DigitalTwin: Digital Twins are the projects of ODTP. These project are owned by users. A Digital Twin can have zero or more Executions</li> <li>An Execution: is a Workflow with Configuration and Run time. It consists of multiple Steps</li> <li>A Step corresponds to exactly one Version of a Component in the Workflow. ODTP builda a Docker image for that step and runs it as a Docker container</li> </ul> <pre><code>---\ntitle: Mongo DB Schema for Execution Definitions\n---\nerDiagram\n    User ||..o{ DigitalTwin : owns\n    DigitalTwin ||..o{ Execution : has\n    Execution }|..|| Workflow : implements\n    Execution ||..|{ Step : has\n    Step ||..|| ComponentVersion : runs\n    Workflow ||..|{ ComponentVersion : has</code></pre>"},{"location":"orchestrator/schema/#er-diagram-for-running-executions","title":"ER Diagram for running executions","text":"<p>The third ER diagram describes all mongodb collections that relate to the running of an execution: These collection capture the output of the execution run</p> <ul> <li>A DigitalTwin can have zero or more Executions</li> <li>An Execution can have one or more Steps</li> <li>A Step has zero or one Output: Outputs are Outputs of a Step. Some steps don't have outputs, see component types</li> <li>An Output links to an output snapshot on S3</li> <li>An Logs: logs can optionally stored in the database</li> <li>A Result: Result is a shared results of a Digital Twin that combines outputs of several executions</li> </ul> <pre><code>---\ntitle: Mongo DB Schema for Execution Runs\n---\nerDiagram\n    DigitalTwin ||..o{ Execution : has\n    Execution ||..|{ Step : has\n    Step ||..o| Output : has\n    Step ||..o| Logs : has\n    Result ||..o{ Output : has\n    Result ||..|{ Execution : compares\n    DigitalTwin ||..o| Result : has</code></pre>"},{"location":"orchestrator/schema/#current-mongodb-schema","title":"Current MongoDB Schema","text":""},{"location":"orchestrator/schema/#users","title":"Users","text":"<pre><code>users = {\n    \"_id\": ObjectId(),\n    \"displayName\": \"John Doe\",\n    \"email\": \"john@example.com\",\n    \"github\": \"johnDoeRepo\",\n    \"created_at\": datetime.utcnow(),\n    \"updated_at\": datetime.utcnow(),\n    \"digitalTwins\": [ObjectId()],\n    \"deprecated\": false,\n}\n</code></pre>"},{"location":"orchestrator/schema/#digital-twins","title":"Digital Twins","text":"<pre><code>digitalTwins = {\n    \"_id\": ObjectId(),\n    \"userRef\": ObjectId(),\n    \"name\" : \"title\",\n    \"status\": \"active\",\n    \"public\": True,\n    \"created_at\": datetime.utcnow(),\n    \"updated_at\": datetime.utcnow(),\n    \"executions\": [ObjectId()],\n    deprecated: false\n}\n</code></pre>"},{"location":"orchestrator/schema/#components","title":"Components","text":"<p>Components are the building blocks of ODTP that correspond to github repos</p> <pre><code>components = {\n    \"_id\": ObjectId(),\n    \"author\": \"Test\",\n    \"componentName\": \"ComponentX\",\n    \"repoLink\": \"https://github.com/odtp-org/odtp-component-example\",\n    \"status\": \"active\",\n    \u201ctype\u201d: \u201cpersistent\u201d,\n    \"description\": \"Description for ComponentX\",\n    \"tags\": [\"tag1\", \"tag2\"],\n    \"created_at\": datetime.utcnow(),\n    \"updated_at\": datetime.utcnow(),\n    \"versions\": [ObjectId()] # 1:n relationship with components,\n    deprecated: false\n}\n</code></pre>"},{"location":"orchestrator/schema/#component-versions","title":"Component Versions","text":"<p>Component versions get their properties from <code>odtp.yml</code> and contain default values for the executions.</p> <pre><code>versions = {\n    \"_id\": ObjectId(),\n    \"Component\u201d: # n:1 relationship with components\n      \u201cComponentId\": ObjectId(),\n      \"componentName\": \"ComponentX\",\n      \"repoLink\": \"https://github.com/odtp-org/odtp-component-example\",\n      \u201ctype\u201d: \u201cpersistent\u201d,\n    }\n    \"version\": \"v1.0\",\n    \"component_version\": \"1.0.0\",\n    \"commitHash\": \"6471218336ce7de41a5162c9556c0ff68f9ec13c\",\n    \"parameters\": {},\n    \"title\": \"Title for Version v1.0\",\n    \"description\": \"Description for Version v1.0\",\n    \"tags\": [\"tag1\", \"tag2\"],\n    \"created_at\": datetime.utcnow(),\n    \"updated_at\": datetime.utcnow(),\n    \"odtp_version\": '0.5.0',\n    \"deprecated\": false,\n    \"imageLink\":  \"https://hub.docker.com/...\",\n    \"description\": \"description from odpt.yml\",\n    \u201ctype\u201d: \u201cpersistent\u201d,\n    \"paramters\": [],\n    \"tags\": [],\n    \"tools\": [],\n    \"licence\": 'AGPL-3.0',\n    \"ports\": [],\n    \"secrets\": [],\n    \"devices\": [],\n    \"data-inputs\": null,\n    \"data-outputs\": null,\n    \"build-args\": null\n}\n</code></pre>"},{"location":"orchestrator/schema/#executions","title":"Executions","text":"<ul> <li><code>dt_id</code> is the reference to the Digital Twin.</li> <li><code>deprecated</code> is a logical delete: the execution will no longer be listed in the GUI.</li> <li><code>workflow_id</code> is the reference to the Workflow</li> </ul> <pre><code>executions = {\n    \"_id\": ObjectId(),\n    \"dt_id\": ObjectId(),\n    \"title\": \"Title for Execution\",\n    \"description\": \"Description for Execution\",\n    \"tags\": [\"tag1\", \"tag2\"],\n    \"workflowSchema\": {\n        \"workflowExecutor\": \"barfi\",\n        \"workflowExecutorVersion\": \"v2.0\",\n        \"component_versions\": [{\"version\": ObjectId()}],\n        \"WorkflowExecutorSchema\": [],\n    },\n    \"start_timestamp\": datetime.utcnow(),\n    \"end_timestamp\": datetime.utcnow(),\n    \"createdAt\": datetime.utcnow(),\n    \"updatedAt\": datetime.utcnow(),\n    \"steps\": [ObjectId()],\n    \"deprecated\": false,\n    \" workflow_id\": ObjectId(),\n    \"execution_path\": '/usr/path/to/execution/directory`\n}\n</code></pre>"},{"location":"orchestrator/schema/#steps","title":"Steps","text":"<p>A Step is a step that was run in an execution.</p> <ul> <li> <p>The field <code>outputs</code> is currently not used, instead the output of a step is a 1-1 relationship and the reference can be found in the <code>output</code> property.</p> </li> <li> <p><code>error</code> and <code>msg</code> will only be filled in case the step errored on its last run. Then it contains the exit message from the docker run command.</p> </li> <li> <p><code>secrets</code> may contain the filename to the encrypted file on the server where the secrets for the step are stored</p> </li> <li> <p><code>run_step</code> determines whether the step will run on the next  execution run. It is possible to partially rerun executions, see tutorial</p> </li> </ul> <pre><code>steps = {\n    \"_id\": ObjectId(),\n    \"executionRef\": ObjectId(),  # Reference to an executions\n    \"timestamp\": datetime.utcnow(),\n    \"start_timestamp\": datetime.utcnow(),\n    \"end_timestamp\": datetime.utcnow(),\n    \"createdAt\": datetime.utcnow(),\n    \"updatedAt\": datetime.utcnow(),\n    \"type\": \"interactive\" or \"ephemeral\",\n    \"logs\": [],\n    \"inputs\": {},\n    \"outputs\": {},\n    \"secrets\": [],\n    \"run_step\": true,\n    \"error\": false,\n    \"msg\": null,\n    \"component_version\": ObjectId(),\n    \"parameters\": {},\n    \"output\": ObjectId(),\n}\n\n### Outputs\n\nOutputs are the output of a step in an execution run. Outputs are stored on S3 and also in a zipped form in the execution project directory.\n\n```json\noutputs = {\n    \"_id\": ObjectId(),\n    \"stepRef\": ObjectId(), # reference to a step,\n    \"output_type\": \"snapshot\" or \"output\",\n    \"s3_bucket\": \"bucket_name\",\n    \"s3_key\": \"path/to/output\",\n    \"file_name\": \"output_file_name\",  # The name of the file in the output\n    \"file_size\": 123456,  # Size of the file in bytes\n    \"file_type\": \"image/jpeg\",  # MIME type or file type\n    \"created_at\": datetime.utcnow(),  # Timestamp when the output was created\n    \"updated_at\": datetime.utcnow(),\n    \"metadata\": {  # Additional metadata associated with the output\n        \"description\": \"Description of the output\",\n        \"tags\": [\"tag1\", \"tag2\"],\n        \"other_info\": \"Other relevant information\"\n    },\n    \"access_control\": {  # Information about who can access this output\n        \"public\": False,  # Indicates if the output is public or private\n        \"authorized_users\": [ObjectId()],\n    }\n}\n</code></pre>"},{"location":"orchestrator/schema/#logs-and-results","title":"Logs and Results","text":"<ul> <li> <p>Logs can optionally stored in the database by the <code>odtp_component_client</code>. We don't recommend this, since it can spam the mongoDB quickly.</p> </li> <li> <p>Results are not yet implemented in ODTP.</p> </li> </ul>"},{"location":"tutorials/","title":"Tutorials overview","text":"<p>The tutorials relate to the ODTP orchestrator In this section you can find tutorials for both the ODTP GUI interface and the ODTP Commandline Interface.</p> <pre><code>flowchart LR\n    A[Getting Started] --&gt; B[Users]\n    B --&gt; G[Digital Twins]\n    G --&gt; C[Define Executions]\n    C --&gt; D[Run Executions]\n    E[Component and Versions] --&gt; F[Workflows]\n    F --&gt; C\n    A --&gt; E</code></pre>"},{"location":"tutorials/#gui-tutorials","title":"GUI Tutorials","text":"<p>GUI Getting Started</p> <ul> <li>Users</li> <li>Digital Twins</li> <li>Components and Versions</li> <li>Workflows</li> <li>Define Executions</li> <li>Run Executions</li> </ul> <p>Alternatively watch the video-tutorials. They are meant to explain the changes from <code>v0.4.0</code> to <code>v00.5.0</code>. But since a lot has changed the videos take you on a full platform tour.</p>"},{"location":"tutorials/#commandline-tutorial","title":"Commandline Tutorial","text":"<p>Commandline Tutorial</p> <p>Note</p> <p>It assumes that you have installed ODTP. If this is not the case: Install ODTP</p>"},{"location":"tutorials/command-line/","title":"Commandline Tutorial","text":"<p>ODTP can also be used via the commandline interface:</p>"},{"location":"tutorials/command-line/#getting-started","title":"Getting Started","text":"<p>If you are using poetry, you can go to <code>odtp</code> folder and run <code>poetry shell</code>. This will load the environment with all dependencies.</p> <p>On the other hand, if you are using the <code>docker-compose</code> solution you can just run <code>odtp</code> command directly.</p> <pre><code>odtp --help\n</code></pre> <p>So you can start working:</p> <p></p>"},{"location":"tutorials/command-line/#component-versions","title":"Component Versions","text":"<p>Component Versions are the building blocks in ODTP: they are used in Workflows and Executions</p> <ul> <li>you need the github repository url</li> <li>you also need a version tag for the version you want to add</li> <li>the github repository needs to be an ODTP component with a valid <code>odtp.yml</code> file</li> </ul> <p>In case the <code>odtp.yml</code> file cannot be parsed you will see error messages:</p> <pre><code>odtp new odtp-component-entry \\\n--component-version v0.1.1 \\\n--repository https://github.com/odtp-org/odtp-component-example\n</code></pre> <p>An error response:</p> <pre><code>\u274c ERROR: Validation error occurred when parsing odtp.yml: Validation failed for odtp.yml:\n('schema-version',): Field required (type: missing)\n('component-authors',): Field required (type: missing)\n('component-repository',): Input should be a valid dictionary or instance of ToolRepository (type: model_type)\n('tools', 0, 'tool-authors'): Field required (type: missing)\n('tools', 0, 'tool-repository'): Input should be a valid dictionary or instance of ToolRepository (type: model_type)\n('devices',): Input should be a valid list (type: list_type)\n</code></pre> <p>When the <code>odtp.yml</code> file can be parsed the output will look like this:</p> <pre><code>\u2705 odtp.yml is valid!\n\u2705 SUCCESS: Component added with details:\n - Component ID: 67ba3cef8a31ac0f177181b9\n - Version ID: 67ba3cef8a31ac0f177181ba\n</code></pre> <p>If the Component Version already exists you will get a warning:</p> <pre><code> WARNING: Component was not added in db: document for repository https://github.com/odtp-org/odtp-component-example and version v0.1.1 already exists\n</code></pre>"},{"location":"tutorials/command-line/#workflows","title":"Workflows","text":"<p>Note</p> <p>In the commandline you don't have to enter a workflow before entering an execution. The workflow will be created automatically when entering an execution.</p> <p>But there is a command to add workflows:</p> <pre><code>odtp new workflow-entry \\\n--name example-workflow \\\n--component-tags odtp-component-example:v0.1.8,odtp-pygwalker:v0.1.6\n</code></pre> <p>The expected output:</p> <pre><code>\u2705 SUCCESS: Workflow has been added:\n - Workflow ID: 67ba3ad6cf79ccdfd1adf6e6\n</code></pre>"},{"location":"tutorials/command-line/#users","title":"Users","text":"<p>So the first thing you need is a user:</p> <ul> <li>the username must be unique</li> <li><code>github</code> should be your valid github username</li> <li><code>email</code> should be your email</li> </ul> <p>Both <code>name</code> and <code>email</code> are used for identification in later steps.</p> <pre><code>odtp new user-entry \\\n--name Max \\\n--email max@mail.com \\\n--github max\n</code></pre> <p>The output should looks like:</p> <pre><code>\u2705 SUCCESS: User with User Id 67bb22f7001a20f7b20347da has been added!\n</code></pre>"},{"location":"tutorials/command-line/#digital-twins","title":"Digital Twins","text":"<p>In the CLI you need the you can add a Digital Twin for a User by providing next to the Digital Twin name the user email:</p> <pre><code>odtp new digital-twin-entry \\\n--user-email max@mail.com \\\n--name example\n</code></pre> <p>The output will look like this:</p> <pre><code>\u2705 SUCCESS: Digital Twin with id 67bb278dc764755ea9438bfe has been added!\n</code></pre>"},{"location":"tutorials/command-line/#executions","title":"Executions","text":"<p>For creating an execution you need:</p> <ul> <li><code>digital-twin-name</code> Name of the digital twin. Alternatively, you can use the <code>digital-twin-id</code>.</li> <li><code>name</code>: A name for the execution</li> <li><code>component-tags</code>: All components tags (<code>componentA:v0.1.0</code>) involved in the workflow aligned sequentially and separated by commas.</li> <li><code>parameter-files</code>: Parameters files separated by commas.<ul> <li>This file should contain all parameters used like in a dotenv file format.</li> </ul> </li> <li><code>ports</code>: Ports matching used by the containers.<ul> <li>Components ports should be separated by <code>,</code>. i.e. <code>8763:3000,8501:8501</code></li> <li>Place as many <code>,</code> as connections between components (steps). If ports are not being used in the first, and second component: i.e. <code>,,8501:8501</code></li> <li>If multiple ports are being use in the same step please use <code>+</code>: i.e. <code>,,8501:8501+3000:3000</code></li> </ul> </li> </ul> <p>A parameter file would look like this: it contains variables and their values:</p> <pre><code>DATASET=rf3\nCATEGORY=Health\nBATCH_SIZE=100\n</code></pre> <p>An execution can be added like this: in case the workflow does not exist yet, it will be created automatically when the execution is added:</p> <pre><code>odtp new execution-entry \\\n--digital-twin-name example-workflow \\\n--name execution-example \\\n--component-tags odtp-dataloader:v1.0.1,odtp-data-dashboard:v1.2.0 \\\n--parameter-files /path/params1,path/params2 \\\n--ports 80:80,8501:8501+80:80\n</code></pre> <p>The output should look like this:</p> <pre><code>\u2705 SUCCESS: execution has been added: see above for the details.\n - execution id: 67bb3139864fecb07899dd68\n - step_ids: [ObjectId('67bb3139864fecb07899dd69'), ObjectId('67bb3139864fecb07899dd6a')]\n</code></pre>"},{"location":"tutorials/command-line/#prepare-execution","title":"Prepare Execution","text":"<p>Next you need to prepare the execution. This will generate all the folder structure and build all necessary docker images for our digital twin. ODTP will check for available images before building, if no image is available then the repository will be pulled and the docker image will be built.</p> <p>An empty folder must be provided to generate the data folder required, and we recommend placing it in a preconfigured digital twin folder.</p> <pre><code>odtp execution prepare \\\n--execution-name example \\\n--project-path /usr/odtp/project/path/for/example\n</code></pre> <p>odtp execution prepare \\ --execution-name dt-example-execution-3 \\ --project-path /Users/smaennel/WORK/ODTP/odtp/docker-compose/digital-twins/test-example2</p> <p>Expected output end:</p> <pre><code>\u2705 SUCCESS: images for the execution have been build!\n</code></pre> <p>In case the project-path is not empty at the start you get:</p> <pre><code>\u26a0\ufe0f WARNING: Project path exists and is not an empty directory\n</code></pre> <p>After this the project path should have the Component repository downloaded with a directory for each step:</p> <pre><code>test-example2\n\u251c\u2500\u2500 0_odtp-component-example_v0.1.8\n\u2502   \u251c\u2500\u2500 odtp-input\n\u2502   \u251c\u2500\u2500 odtp-logs\n\u2502   \u251c\u2500\u2500 odtp-output\n\u2502   \u2514\u2500\u2500 repository\n\u2514\u2500\u2500 1_odtp-pygwalker_v0.1.6\n    \u251c\u2500\u2500 odtp-input\n    \u251c\u2500\u2500 odtp-logs\n    \u251c\u2500\u2500 odtp-output\n    \u2514\u2500\u2500 repository\n</code></pre>"},{"location":"tutorials/command-line/#run-execution","title":"Run Execution","text":"<p>Once your execution is prepared, it's time to run it! When running an execution you can provide some secrets for your components separated by commas (<code>,</code>) similar to how you define the pipeline in the execution generation. Secrets files are structure in a similar way to parameters files: they contain keys and values:</p> <pre><code>password=mypassword\n</code></pre> <pre><code>odtp execution run \\\n--execution-name execution-example \\\n--secrets-files /path/Secrets001,/path/Secrets001 \\\n</code></pre> <p>This will run the docker containers. They will exchange data via S3 snapshots.</p> <p>You can find logs on the <code>project-path</code> that you specified in the <code>execution prepare</code> command</p> <pre><code>/Users/smaennel/WORK/ODTP/odtp/docker-compose/digital-twins/test-example\n\u251c\u2500\u2500 0_odtp-component-example_v0.1.8\n\u2502   \u251c\u2500\u2500 odtp-input\n\u2502   \u251c\u2500\u2500 odtp-logs\n\u2502   \u2502   \u251c\u2500\u2500 log.txt\n\u2502   \u2502   \u251c\u2500\u2500 odtpLoggerDebugging.txt\n\u2502   \u2502   \u2514\u2500\u2500 odtpS3UploadedDebugging.txt\n\u2502   \u251c\u2500\u2500 odtp-output\n\u2502   \u2502   \u251c\u2500\u2500 odtp-logs.zip\n\u2502   \u2502   \u251c\u2500\u2500 odtp-output.zip\n\u2502   \u2502   \u2514\u2500\u2500 rotten_tomatoes\n\u2502   \u2502       \u251c\u2500\u2500 test.csv\n\u2502   \u2502       \u251c\u2500\u2500 train.csv\n\u2502   \u2502       \u2514\u2500\u2500 validation.csv\n\u2502   \u2514\u2500\u2500 repository\n\u2514\u2500\u2500 1_odtp-pygwalker_v0.1.6\n    \u251c\u2500\u2500 odtp-input\n    \u2502   \u2514\u2500\u2500 rotten_tomatoes\n    \u2502       \u251c\u2500\u2500 test.csv\n    \u2502       \u251c\u2500\u2500 train.csv\n    \u2502       \u2514\u2500\u2500 validation.csv\n    \u251c\u2500\u2500 odtp-logs\n    \u251c\u2500\u2500 odtp-output\n    \u2514\u2500\u2500 repository\n</code></pre>"},{"location":"tutorials/command-line/#example-use-case-with-cli-commands","title":"Example Use Case with CLI commands:","text":"<p>See dt-example workflow\u00a0\u29c9. It includes a script with the CLI commands <code>dt-example.sh</code></p>"},{"location":"tutorials/command-line/#list-documents-from-the-mongodb","title":"List Documents from the MongoDB","text":"<p>You can use the following commands to get an overview of the documents in ODTP: it takes as argument the collection names, see [../orchestrator/schema.md]</p> <pre><code>odtp db ls executions\n</code></pre> <pre><code>+--------------------------+------------------------+---------------------------+----------------------------+---------------+--------------------------+\n|           _id            |         title          |        description        |      start_timestamp       | end_timestamp |      digitalTwinRef      |\n+--------------------------+------------------------+---------------------------+----------------------------+---------------+--------------------------+\n| 67a9c69149d18fbe0f1ee424 |      test-example      | Description for Execution | 2025-02-14 06:46:27.458000 |      None     | 67a9c66f49d18fbe0f1ee3f8 |\n| 67aee703b4c8a9d770b0de62 |         test-2         | Description for Execution | 2025-02-14 06:47:49.016000 |      None     | 67a9c66f49d18fbe0f1ee3f8 |\n| 67b99408f74e0dbe2b93e555 |    test-dt-example     | Description for Execution |            None            |      None     | 67a9c66f49d18fbe0f1ee3f8 |\n| 67b9bb7df74e0dbe2b93e588 |         test4          | Description for Execution | 2025-02-22 14:57:30.892000 |      None     | 67a9c66f49d18fbe0f1ee3f8 |\n| 67b9bc25f74e0dbe2b93e5dd |    test dataloader     | Description for Execution |            None            |      None     | 67a9c66f49d18fbe0f1ee3f8 |\n| 67bb3139864fecb07899dd68 | dt-example-execution-3 | Description for Execution | 2025-02-23 15:58:23.671000 |      None     | 67bb31389f79ef56cba361a3 |\n+--------------------------+------------------------+---------------------------+----------------------------+---------------+--------------------------+\n</code></pre>"},{"location":"tutorials/command-line/#delete-an-execution","title":"Delete an execution","text":"<p>Executions and all associated data, such as MongoDB entries, S3 Files, and project path folders can be easily deleted. In <code>v0.5.0</code> this feature is only available in the CLI. In the GUI Executions can be deprecated instead.</p> Command Line CLI <pre><code>odtp execution delete \\\n--execution-name execution-example \\\n--project-path /path/exeuction\n</code></pre> <p>Existing executions can be selected in order to run them: when you select an execution you see a button: \"PREPARE AND RUN EXECUTION\": click on it and you will get to a run page where you can run the execution: see run executions</p>"},{"location":"tutorials/component-versions/","title":"Components and Versions","text":"<p>Note</p> <ul> <li>Components are code repositories that wrap tools, see Components</li> <li>Components come with Versions that correspond to tags in the github repository</li> <li>When Component Versions are added then the <code>odtp.yml</code> is parsed: it needs to conform to the standards described at odtp.yml format</li> <li>The building blocks in ODTP are Component Versions</li> <li>Component Versions can also be added via the Commandline</li> </ul>"},{"location":"tutorials/component-versions/#manage-component-versions","title":"Manage Component Versions","text":"<p>In the Dashboard you can list all Component Versions that are already on ODTP.</p> <ul> <li>you can deprecate Component Versions</li> <li>you can also reactivate them</li> <li>you can chose to also show deprecated Component Versions. By default they are not displayed</li> </ul> <p></p>"},{"location":"tutorials/component-versions/#add-a-new-component-version","title":"Add a new component version","text":"<p>In order to start using the components in ODTP you need to add them to the platform. Components are the building block in workflows and executions.</p> <ul> <li>Components are imported by their github url and Version correspond to the version tags.</li> <li>A component repository should have a valid tag in order to be imported.</li> <li>A component version also needs to have a valid <code>odtp.yml</code>. Otherwise it can not be imported into ODTP: see odtp.yml specification</li> </ul> <p>Under the tab \"Add Component Version\" you can either add a new component with a component version or a new version for an existing component.</p> <p></p> <p></p> <p>You will see a message when ODTP was able to parse the <code>odtp.yml</code> file</p> <p></p> <p>Error case: <code>odtp.yml</code> file could not be parsed:</p> <p></p>"},{"location":"tutorials/component-versions/#component-version-details","title":"Component Version Details","text":"<p>The component version gets the default configuration and other metadata from the <code>odtp.yml</code> file: you can see this information in the detail tab:</p> <p></p>"},{"location":"tutorials/component-versions/#next-add-workflow","title":"Next add Workflow","text":"<p>Once you have added all the component versions you need, you can combine them into Workflows:</p> <p>Workflows</p>"},{"location":"tutorials/digital-twins/","title":"Users and digital twins","text":"<p>Note</p> <ul> <li>Digital Twins are the projects: they are owned by a user</li> <li>The digital twin name has to be unique among the digital twins of a user.</li> <li>Digital Twins can also be added via the Commandline</li> </ul>"},{"location":"tutorials/digital-twins/#setup-a-digital-twin","title":"Setup a Digital Twin","text":"<p>Once you have a user, you can set up a digital twin</p> <p></p>"},{"location":"tutorials/digital-twins/#select-a-digital-twin","title":"Select a Digital Twin","text":"<p>Now you can select your digital twin. Once you have selected a digital twin you can set up executions</p> <p></p>"},{"location":"tutorials/digital-twins/#manage-digital-twins","title":"Manage Digital Twins","text":"<p>You can also manage your digital twins.</p> <p></p>"},{"location":"tutorials/digital-twins/#next-add-executions","title":"Next Add Executions","text":"<p>Once you have added a Digital Twin you can add Executions:</p> <p>Executions</p>"},{"location":"tutorials/executions/","title":"Executions","text":"<p>Note</p> <ul> <li>Executions are owned by users and belong to a digital twin</li> <li>Executions build on Workflows adding configurations</li> <li>Executions have Steps that correspond to Component Versions</li> <li>The output of one Step will serve as input to the next Step</li> <li>Executions can also be added on the Commandline</li> <li>Execution can be created with defaults or they  can be configured. (Defaults are taken from the Workflows)</li> </ul> <pre><code>graph TB\nA[component-example_0.1.0 + parameters + port mappings + secret files]\nB[travel_dashboard_0.2.1 + parameters + port mappings + secret files]\nA --&gt; B;</code></pre>"},{"location":"tutorials/executions/#add-an-execution","title":"Add an execution","text":"<p>You can add an execution on the \"Add an Execution\" tab:</p> <p></p> <p>An execution has defaults set in the <code>odtp.yml</code> file</p> <p>Note</p> <ul> <li>You can also create the Execution with the defaults and configure it later when you run</li> <li>Or you can configure it during creation: some configurations such as a complete change of parameters are only possible during the creation of an execution</li> </ul>"},{"location":"tutorials/executions/#execution-with-defaults","title":"Execution with Defaults","text":"<p>The Execution has the defaults already set:</p> <p></p> <p>For each Component Version you can expand the Component Version Info above the Step.</p> <p></p>"},{"location":"tutorials/executions/#available-configurations","title":"Available Configurations","text":"<p>For each Step you can configure:</p> <ul> <li>parameters</li> <li>port mappings</li> <li>add secret files</li> </ul>"},{"location":"tutorials/executions/#parameters","title":"Parameters","text":"<p>You can upload parameters from a file: this will overwrite the existing parameters with out additional checks. The upload area need to be expanded above the Step on the right.</p> <p>A parameter file would look like this: so it contains keys and values:</p> <pre><code>DATASET=rf3\nCATEGORY=Health\nBATCH_SIZE=100\n</code></pre> <p>Note</p> <ul> <li>Parameter uploads are only offered when you create the Execution</li> <li>When you run the Execution you can only overwrite the existing parameters</li> </ul> <p></p> <p></p> <p></p>"},{"location":"tutorials/executions/#port-mappings","title":"Port Mappings","text":"<p>You can overwrite port mappings</p> <p></p>"},{"location":"tutorials/executions/#secrets-files","title":"Secrets Files","text":"<p>In case Secrets are needed you are offered a selection of your uploaded encrypted secrets file. If you don't have them uploaded yet, see how to uploaded encrypted secrets files.</p> <p></p> <p></p>"},{"location":"tutorials/executions/#save-execution","title":"Save Execution","text":"<p>Don't forget to save the execution by clicking on \"Save Execution\". Alternatively you can also clear the screen and click on \"Reset Execution\".</p> <p></p>"},{"location":"tutorials/executions/#manage-executions","title":"Manage Executions","text":"<p>The Execution page assumes that you have selected both a user and a digital twin to work on.</p> <p>In the Dashboard you can list all Executions of your selected Digital Twin.</p> <ul> <li>you can deprecate Executions</li> <li>you can also reactivate them</li> <li>you can chose to also show deprecated Executions. By default they are not displayed</li> </ul> <p></p>"},{"location":"tutorials/executions/#next-run-execution","title":"Next Run Execution","text":"<p>Once you have added an Execution you can run it:</p> <p>Run Executions</p>"},{"location":"tutorials/getting-started/","title":"Getting started","text":"<p>This tutorial will guide you through setting up a digital twin and running it</p>"},{"location":"tutorials/getting-started/#start","title":"Start","text":"<p>If you are using poetry, you can go to <code>odtp</code> folder and run <code>poetry shell</code>. This will load the environment with all dependencies.</p> <p>On the other hand, if you are using the <code>docker-compose</code> solution you can just run <code>odtp</code> command directly.</p> <pre><code>odtp dashboard\n</code></pre>"},{"location":"tutorials/getting-started/#now-the-interface-should-be-up","title":"Now the interface should be up","text":"<p>So you can start working:</p> <p></p>"},{"location":"tutorials/getting-started/#next-for-shared-building-blocks","title":"Next for shared building blocks","text":"<p>The shared building blocks start with Component Versions:</p> <p>Component Versions</p>"},{"location":"tutorials/getting-started/#next-for-user-owned-projects","title":"Next for user owned projects","text":"<p>The user journey starts at Users:</p> <p>Users</p>"},{"location":"tutorials/run-executions/","title":"Run Executions","text":"<p>Note</p> <ul> <li>Run Executions by selecting them</li> <li>Partial runs of executions are possible</li> <li>You can also run executions via the commandline</li> </ul> <pre><code>graph TB\nA[component-example_0.1.0 + parameters + port mappings + secret files]\nB[travel_dashboard_0.2.1 + parameters + port mappings + secret files]\nA --&gt; B;</code></pre> <p>Go to the tab \"Execution Run\".</p>"},{"location":"tutorials/run-executions/#select-an-execution-for-run","title":"Select an Execution for Run","text":"<p>Select a new execution for a run and you will see how it has been set up in each step:</p> <ul> <li>parameters</li> <li>port mappings</li> <li>secret files</li> </ul> <p></p> <p></p>"},{"location":"tutorials/run-executions/#step-1-adapt-configuration","title":"Step 1: Adapt Configuration","text":"<p>You can still overwrite the configuration:</p>"},{"location":"tutorials/run-executions/#overwrite-parameter-values","title":"Overwrite Parameter Values","text":"<p>You can overwrite the parameter values, but not upload parameters from file as when an execution is created</p> <p></p>"},{"location":"tutorials/run-executions/#overwrite-port-mappings","title":"Overwrite Port Mappings","text":"<p>You can overwrite port mappings</p> <p></p>"},{"location":"tutorials/run-executions/#select-secrets-files","title":"Select Secrets Files","text":"<p>You can select files with the secrets that the step need similar to when the execution is created</p> <p></p>"},{"location":"tutorials/run-executions/#step-3-create-execution-directory","title":"Step 3: Create Execution Directory","text":"<p>Next you need to create an execution directory by clicking on the button at the bottom of the page: \"Create Project Directory\"</p> <p></p> <p>You will see a new button \"Prepare Execution\" appear.</p>"},{"location":"tutorials/run-executions/#step-4-prepare-the-execution","title":"Step 4: Prepare the Execution","text":"<p>Click on the button and it will start to build the images. When possible previously build images will be reused. You will see the terminal output in a window.</p> <p></p> <p>Once the images have been build you will see a new button \"Run execution\" appear.</p>"},{"location":"tutorials/run-executions/#step-5-run-execution","title":"Step 5: Run Execution","text":"<p>Again you will see the terminal output in a window: in the example below the component run failed.</p> <p></p> <p>Once you close the terminal window, you will also be able to see the error on the execution run page.</p> <p> </p>"},{"location":"tutorials/run-executions/#step-6-optional-rerun-a-step","title":"Step 6 (Optional) Rerun a Step","text":"<p>In case a Step failed to run you can make correction and switch a flag to rerun the Step.</p> <p> </p> <p>Interactive Components stay running, so in that case you won't see a success message in the terminal output window. You may also take a look on docker to see what is happening:</p> <p></p> <p>Since the Step is running as a container, we can close the Terminal output window and take a look on the page again:</p>"},{"location":"tutorials/run-executions/#step-7-check-the-logs","title":"Step 7 Check the logs","text":"<p>Each step may have a button to check the logs.</p> <p></p> <p>Typical log of an ephermeral Step: the Component ran and ended.</p> <p></p> <p>Typical log of an interactive Step: the application started and is served.</p> <p></p>"},{"location":"tutorials/users/","title":"Users","text":"<p>Note</p> <ul> <li>The username has to be unique and the github user has to exist.</li> <li>Users can also be added via the Commandline</li> </ul>"},{"location":"tutorials/users/#setup-a-user","title":"Setup a user","text":"<p>So the first thing you need is a user:</p> <p></p>"},{"location":"tutorials/users/#select-a-user","title":"Select a User","text":"<p>As a next step you can select the user and add additional information:</p> <p></p>"},{"location":"tutorials/users/#upload-secrets","title":"Upload Secrets","text":"<p>You can upload named files with secrets.</p> <p>Note</p> <ul> <li>The secret files are stored encrypted on the server.</li> <li>Once you uploaded secrets the application will test whether it can do the encryption and show you the keys that are stored in the file.</li> </ul> <p></p> <p></p> <p></p> <p></p>"},{"location":"tutorials/users/#add-a-digital-twin","title":"Add a Digital Twin","text":"<p>Once you have a user, you can set up a Digital Twin.</p> <p>Digital Twins</p>"},{"location":"tutorials/video-tutorials/","title":"Video Tutorials","text":"<p>Note</p> <p>Click on fullscreen to watch the videos on fullscreen</p>"},{"location":"tutorials/video-tutorials/#example-pipeline","title":"Example pipeline","text":"<p>The video takes you through the setup for an example pipeline and explains what is needed for the execution run:</p>"},{"location":"tutorials/video-tutorials/#corsica-pipeline","title":"Corsica pipeline","text":"<p>The video takes you through the setup of the corsica pipeline, that is more complex to setup and also needs the uploads of secrets:</p>"},{"location":"tutorials/workflows/","title":"Workflows","text":"<p>Note</p> <ul> <li>in the CLI workflows are added automatically when executions are added</li> <li>workflows are templates for executions that determine the sequence of Component Versions to be run, without yet specifying the parameters or run time</li> <li>workflows are shared between users and not owned by users</li> </ul> <pre><code>graph TB\nA[component-example_0.1.0]\nB[travel_dashboard_0.2.1]\nA --&gt; B;</code></pre>"},{"location":"tutorials/workflows/#manage-workflows","title":"Manage Workflows","text":"<p>In the Dashboard you can list all Workflows that are already on ODTP.</p> <ul> <li>you can deprecate Workflows</li> <li>you can also reactivate them</li> <li>you can chose to also show deprecated Workflows. By default they are not displayed</li> </ul> <p></p>"},{"location":"tutorials/workflows/#add-a-workflow","title":"Add a Workflow","text":"<p>In the GUI you need to define workflows before you can add executions. In the CLI workflows will be added automatically when you add an execution.</p> <p></p>"},{"location":"tutorials/workflows/#workflow-details","title":"Workflow Details","text":"<p>You can view the workflow with its components on the Workflow detail tab:</p> <p></p>"},{"location":"tutorials/workflows/#next-add-executions","title":"Next add Executions","text":"<p>A Workflow is the template for executions: Executions are Workflows with added configurations and actual runs. So once you have added a workflow, you can Executions for it:</p> <p>Executions</p>"},{"location":"usecases/","title":"ODTP Use Cases","text":"<p>The development of ODTP is has happened in parallel to the adaptation of different use cases. In this section we list some example use cases from the mobility domain. These use cases came out of the cooperation between CSFM and SDSC, see Acknowledgement and Funding.</p>"},{"location":"usecases/#list-of-use-cases","title":"List of Use Cases","text":"<ul> <li>Mobility: Eqasim and Matsim Simulation</li> <li>Mobility: Causal Inference</li> </ul>"},{"location":"usecases/mobility-causal-interventions/","title":"Causal Interventions","text":""},{"location":"usecases/mobility-causal-interventions/#description","title":"Description","text":"<p>We provide another workflow for a digital twin that implements the mobility causal intervention framework to evaluate the robustness of deep learning models towards data distribution shifts, with the application of individual next location prediction <sup>1</sup>.</p>"},{"location":"usecases/mobility-causal-interventions/#digital-twin-workflow","title":"Digital Twin Workflow","text":"<p>Overview of the mobility causal intervention workflow in ODTP:</p> <pre><code>graph LR\n    ExternalDatabase[External Database] --&gt; SQLDataloader[SQL Dataloader]\n    ExternalDatabase --&gt; PostGisDataloader[Post-GIS Dataloader]\n\n    subgraph ODTP\n        SQLDataloader --&gt; ODTPMobilitySimulation[ODTP Mobility Simulation]\n        PostGisDataloader --&gt; ODTPMobilitySimulation\n        ODTPMobilitySimulation --&gt; ODTpMetrics[ODTP Metrics]\n        ODTPMobilitySimulation --&gt; ODTpNextLocationPrediction[ODTP Next Location Prediction]\n    end\n\n    Model[Model] --&gt; ODTpNextLocationPrediction</code></pre> <p>The mobility simulation module is used to generate individual location sequences. It also incorporates the causal intervention mechanism to generate intervened synthetic data that represent different data distribution shifts. These synthetic data are fed into the next-location-prediction module to quantify a model\u2019s robustness against interventions. Meanwhile, a mobility-metrics module is used to monitor the change in the characteristics of mobility data.</p>"},{"location":"usecases/mobility-causal-interventions/#overview-of-the-components","title":"Overview of the components","text":"<ul> <li>odtp-postgresql-dataloader\u00a0\u29c9: TBD</li> <li>odtp-sql-dataloader\u00a0\u29c9: This component performs SQL queries to a compatible database and create a dataframe output in csv format. </li> <li>odtp-mobility-simulation\u00a0\u29c9: This module generates synthetic individual location visit sequences based on mechanistic mobility simulators (including EPR, IPT, Density-EPR, and DT-EPR models). The module also generates intervened synthetic mobility data based on causal interventions through the specification of parameters to be intervened and levels of the interventions.</li> <li>odtp-mobility-metrics\u00a0\u29c9: This module includes multiple metrics to quantify the characteristics of individual mobility sequences, e.g., location visitation frequency, radius of gyration, real entropy, mobility motifs etc. </li> <li>odtp-next-location-prediction\u00a0\u29c9: This module includes two deep learning models for individual next location prediction, the LSTM model and the Multi-Head Self-Attentional (MHSA) model. </li> </ul>"},{"location":"usecases/mobility-causal-interventions/#contact","title":"Contact","text":"<ul> <li>CSFM\u00a0\u29c9 or SDSC\u00a0\u29c9</li> </ul> <ol> <li> <p>Hong, Y., Xin, Y., Dirmeier, S., Perez-Cruz, F., &amp; Raubal, M. (2023). Revealing behavioral impact on mobility prediction networks through causal interventions. arXiv preprint arXiv:2311.11749.\u00a0\u21a9</p> </li> </ol>"},{"location":"usecases/mobility-eqasim-matsim/","title":"Eqasim and Matsim simulations","text":""},{"location":"usecases/mobility-eqasim-matsim/#description","title":"Description","text":"<p>Digital Twin involving Mobility Simulation of three scenarios: <code>Ile de France</code>, <code>Corsica</code>, and <code>Switzerland</code>. </p>"},{"location":"usecases/mobility-eqasim-matsim/#digital-twin-workflow","title":"Digital Twin Workflow","text":"<p>This workflow involves 4 components:</p> <pre><code>graph LR\n    data_source --&gt; odtp-eqasim-dataloader \n    subgraph ODTP\n        odtp-eqasim-dataloader --&gt; odtp-eqasim\n        odtp-eqasim --&gt; odtp-matsim\n        odtp-matsim --&gt; odtp-travel-data-dashboard\n    end    </code></pre> <ul> <li>odtp-eqasim-dataloader\u00a0\u29c9: The data loader component prepares statistical data on the population of the respective scenario and the geographic data from standardized data sources. </li> <li>odtp-eqasim\u00a0\u29c9: The eqasim component generates a synthetic population based on statistical data and links them with travel profiles according to statistical properties.</li> <li>odtp-eqasim-matsim\u00a0\u29c9: The MATSim component uses the synthetic population to generate transport simulations.</li> <li>odtp-travel-data-dashboard\u00a0\u29c9: The travel data dashboard visualizes the Origin-Destination data output of the MATSim simulation to communicate mobility patterns.</li> </ul>"},{"location":"usecases/mobility-eqasim-matsim/#corsica-use-case","title":"Corsica Use Case","text":"<p>Note</p> <p>The Corsica workflow repo is still under construction and is not yet public </p> <p>As the data for the corsica use case is open data, there is also a repo sharing its workflow: Corsica Workflow and Tutorial\u00a0\u29c9</p> <pre><code>graph LR\n    SwitchDatabase[Switch Database] --&gt; EqasimDataloader[Eqasim Dataloader]\n\n    subgraph ODTP\n        EqasimDataloader --&gt; Eqasim\n        Eqasim --&gt; EqasimMatsim[Eqasim-Matsim]\n        EqasimDataloader --&gt; EqasimMatsim\n        EqasimMatsim --&gt; TravelDataDashboard[Travel Data Dashboard]\n    end</code></pre>"},{"location":"usecases/mobility-eqasim-matsim/#reference-and-contact","title":"Reference and contact","text":"<ul> <li>CSFM\u00a0\u29c9 or SDSC\u00a0\u29c9</li> <li>The French scenario are publicly available, the Swiss scenario requires a valid contract with the Swiss Federal Statistics Office FSO\u00a0\u29c9.</li> </ul>"},{"location":"workflows/","title":"Workflow Concept","text":""},{"location":"workflows/#workflow-terminology","title":"Workflow Terminology","text":"<ul> <li>Components: The ODTP framework allows to combine independently developed tools into pipelines by wrapping the tools and turning them into ODTP components. Components have default parameters that are described in the <code>odtp.yml</code> file.</li> <li>Workflows: Components can then be combined into pipelines by the ODTP orchestrator. These pipelines are then called Workflows. Workflows specify the sequence of components before the parameters are set and before a run time has been decided. They are reusable in the same way that components are reusable.</li> <li>Executions: An execution of a Workflow happens when at a specific date and time a workflow is run with parameters, ports and if necessary secrets. The execution is build and run as a sequence of docker containers. The outputs and results of Executions are captured by the ODTP orchestrator.</li> </ul> <p>Note</p> <ul> <li>Components and Executions are not owned by users as they are abstract objects</li> <li>Digital Twins and Executions are owned by user</li> <li>Executions execute a workflow with a certain configuration, that is specified by parameters, ports and secrets</li> <li>Executions can be repeated as partial runs: times will then be overwritten and the output might change</li> <li>Digital Twins are collections of Executions</li> </ul> <pre><code>graph TB;\n    subgraph Shared building blocks: Components and Workflows\n        direction LR\n        subgraph CS[Components]\n            direction TB\n            A[Component A]\n            B[Component B]\n            C[Component C]\n            D[Component D]\n            E[Component E]\n        end\n        CA0[Component A]\n        CB0[Component B]\n        CC0[Component C]\n        CD0[Component D]\n        CE0[Component E]\n        subgraph Workflows\n            direction TB\n            subgraph W1[Workflow1]\n                direction LR\n                CA0 --&gt; CB0\n                CB0 --&gt; CC0\n            end\n            subgraph W2[Workflow2]\n                direction LR\n                CD0 --&gt; CE0\n            end\n        end\n    end\n    style CS fill:white\n    style W1 fill:white\n    style W2 fill:white</code></pre> <pre><code>graph TB;\n    subgraph Execution of Workflows\n        direction TB\n        CA1[Component A]\n        CB1[Component B]\n        CC1[Component C]\n        CA2[Component A &lt;br/&gt;with Configuration &lt;br/&gt;]\n        CB2[Component B &lt;br/&gt;with Configuration &lt;br/&gt;]\n        CC2[Component C &lt;br/&gt;with Configuration &lt;br/&gt;]\n        subgraph W[Shared: Workflow]\n            direction LR\n            CA1 --&gt; CB1\n            CB1 --&gt; CC1\n        end\n        subgraph E[User Owned: Execution: run at time, steps can be repeated]\n            direction LR\n            CA2 --&gt; CB2\n            CB2 --&gt; CC2\n        end\n        W -.- t[use as blueprint, add configuration and execution time] -.-&gt; E\n    end\nstyle W fill:white\nstyle E fill:white</code></pre>"},{"location":"workflows/#workflow-examples","title":"Workflow Examples","text":"<p>See the use cases for workflow examples</p>"},{"location":"workflows/#workflow-structure","title":"Workflow Structure","text":"<p>Currently the ODTP orchestrator supports only linear workflows. But it is planned to also support acyclic graphs in the future, see our roadmap</p> <p>Note</p> <p>If your pipeline involves loading multiple data inputs into a single step, you can solve this by adding all of them sequentially and activating the environment variable <code>TRANSFER_INPUT_TO_OUTPUT=TRUE</code> in the Dataloader. This will transfer the input from one dataloader to the next, accumulating all inputs for the analytical component.</p>"},{"location":"zoo/","title":"ODTP Zoo","text":"<p>Under Construction</p> <p>The Zoo is available\u00a0\u29c9. It is still under construction and also not yet connected to the ODTP Orchestrator</p>"},{"location":"zoo/#what-is-a-odtp-zoo","title":"What is a ODTP zoo?","text":"<pre><code>graph TD;\n    subgraph ODTPOrchestrator\n    CLI[CLI]\n    GUI[GUI]\n    end\n    subgraph ODTPZoo\n    Registery[index.yml/index.json]\n    end   \n    ODTPOrchestrator --&gt;|use| ODTPComponentA\n    ODTPOrchestrator --&gt;|use| ODTPComponentB\n    ODTPOrchestrator --&gt;|use| ODTPComponentC\n    ODTPComponentA --&gt;|register| ODTPZoo\n    ODTPComponentB --&gt;|register| ODTPZoo\n    ODTPComponentC --&gt;|register| ODTPZoo\n    subgraph ODTPZoo[Component Registry]\n    end\n    subgraph ODTPComponentA[ComponentA]\n    end\n    subgraph ODTPComponentB[ComponentB]\n    end\n    subgraph ODTPComponentC[ComponentC]\n    end    </code></pre> <p>A ODTP zoo is a collection of ODTP Components that have been validated and can therefore be used by the ODTP Orchestrator to build Executions.</p>"},{"location":"zoo/#how-it-works","title":"How it works:","text":"<p>Components are registered in the zoo by an index file. The goal is to make them discoverable via their metadata.</p>"},{"location":"zoo/#odtp-zoos","title":"ODTP Zoos","text":"<p>Currently there is just one ODTP zoo at for the ODTP organization</p> <ul> <li>ODTPOrg Zoo\u00a0\u29c9</li> </ul> <p>There could be more ODTP zoos. As not all components are open source, organizations could build their own ODTP zoos that provide components that can be used within their organizations.</p>"},{"location":"zoo/#getting-started-with-the-zoo","title":"Getting started with the zoo","text":"<p>See here for how to add a component to the ODTP-org zoo:</p> <p>Add a component to the ODTP Zoo </p>"},{"location":"zoo/#odtp-zoo-public-page","title":"ODTP Zoo Public Page","text":""},{"location":"zoo/add-component/","title":"Add component to the ODTP-org zoo","text":""},{"location":"zoo/add-component/#locate-the-github-repsitory-of-the-zoo","title":"Locate the github repsitory of the zoo.","text":"<p>The zoo db is located in a github repository: ODTP-org Zoo\u00a0\u29c9</p> <p>In order to add a component you must submit it via a PR. It will then be added to <code>index.json</code>, that lists all available components with their metadata. </p>"},{"location":"zoo/add-component/#how-to-submit-a-component","title":"How to submit a component","text":"<p>In order to submit a component you need to make an entry in the <code>components</code> directory using <code>component_template.yaml</code>. Please fill the template with all requested information and then open a pull request. </p>"},{"location":"zoo/add-component/#step-by-step-instructions","title":"Step by step instructions","text":"<ol> <li>Fork the zoo repository.</li> <li>Copy the <code>odtp.yml</code> and rename it as <code>component_version.yaml</code>.</li> <li>Place the file into <code>components</code> directory.</li> <li>Submit a pull request and wait for review. <ul> <li>Components pull requests target <code>components</code> branch, after merging it will get automatically deployed to <code>main</code>.</li> <li>Do not edit <code>index.json</code> directly, and do not modify any other file. </li> <li>The added date will be automatically populated after the merge. </li> </ul> </li> </ol>"},{"location":"zoo/add-component/#notes","title":"Notes","text":"<ul> <li>Only functional components will be accepted. You can make your tool compatible using the <code>odtp-component-template</code>\u00a0\u29c9. </li> <li>If you want to have your component removed, please open an issue or pull request.</li> </ul>"}]}